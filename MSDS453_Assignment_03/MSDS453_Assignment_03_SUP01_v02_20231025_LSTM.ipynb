{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzOQmEXW7gOs"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60GSgcVT7kOl"
      },
      "source": [
        "In the second part of the assignment, we train a deep neural network (LSTM) for multi-class classification on the class corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_u4sgjFAwKG"
      },
      "source": [
        "Text, that is, a sequence of words is a fundamentally different type of data when compared with tabular data or images/videos. For text neither is the input size fixed (e.g., movie reviews of different lengths) nor is the output fixed (e.g., binary classification or translated text). Deep learning with text is hence a case of variable inputs - variable outputs and needs special architectures (e.g., RNN's, Transformers)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MsHy0TlBdVN"
      },
      "source": [
        "The workflow to model text, however, remains the same - convert the documents to a vectorized representation and use this representation for a downstream task (e.g., sentiment analysis)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2RZnrdBvsQXx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FEbiBEI74yr"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gd71brOg7xPi"
      },
      "source": [
        "Let us begin by preparing the data required to build the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3UwiVJ5-s_Ee"
      },
      "outputs": [],
      "source": [
        "data_file = 'https://raw.githubusercontent.com/barrycforever/MSDS_453_Public/main/MSDS453_ClassCorpus/MSDS453_QA_20220906.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lar1_MOGtG2Y"
      },
      "outputs": [],
      "source": [
        "class_corpus = pd.read_csv(data_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grAjtB1WtMR3",
        "outputId": "ede364b2-a862-4b33-b05a-77cd4c5f5eb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 200 entries, 0 to 199\n",
            "Data columns (total 8 columns):\n",
            " #   Column                    Non-Null Count  Dtype \n",
            "---  ------                    --------------  ----- \n",
            " 0   Doc_ID                    200 non-null    int64 \n",
            " 1   DSI_Title                 200 non-null    object\n",
            " 2   Text                      200 non-null    object\n",
            " 3   Submission File Name      200 non-null    object\n",
            " 4   Student Name              200 non-null    object\n",
            " 5   Genre of Movie            200 non-null    object\n",
            " 6   Review Type (pos or neg)  200 non-null    object\n",
            " 7   Movie Title               200 non-null    object\n",
            "dtypes: int64(1), object(7)\n",
            "memory usage: 12.6+ KB\n"
          ]
        }
      ],
      "source": [
        "class_corpus.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2CaQjG2vfxl"
      },
      "source": [
        "# Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IUCz3cI77UL"
      },
      "source": [
        "As an example of multi-class classification, let us look at building a deep neural network to infer the sentiment of a review from the text of the review. Along the way, we will look at the architecture of a recurrent neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UiuLe6Fvr1u"
      },
      "source": [
        "The [latest tensorflow API](https://www.tensorflow.org/tutorials/keras/text_classification) unifies dataset structures for text and images (see screenshot below). However, we do not have the data in this format. We will first bring our data to the format that is requried by Tensorflow. For large scale projects it is beneficial to use this format, since document level errors can be easily localized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkltjBp5v2Bu"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyBpXZn6vkRs"
      },
      "source": [
        "## Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CwnSu7TS066H"
      },
      "outputs": [],
      "source": [
        "root_examples_dir = Path('/content/examples/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AZVrT_JC1B1P"
      },
      "outputs": [],
      "source": [
        "root_examples_dir.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3vrOUerUvips"
      },
      "outputs": [],
      "source": [
        "pos_examples_dir = root_examples_dir / 'positive'\n",
        "neg_examples_dir = root_examples_dir / 'negative'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xZlac4kXwSS0"
      },
      "outputs": [],
      "source": [
        "pos_examples_dir.mkdir(parents=True, exist_ok=True)\n",
        "neg_examples_dir.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "QVFZMGbtvE_u"
      },
      "outputs": [],
      "source": [
        "class_corpus_pos = class_corpus.loc[class_corpus[\"Review Type (pos or neg)\"] == 'Positive', 'Text']\n",
        "class_corpus_neg = class_corpus.loc[class_corpus[\"Review Type (pos or neg)\"] == 'Negative', 'Text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hev64g4x0ES",
        "outputId": "2b84659e-d1d0-42a9-abe2-a8d6d434a46b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((100,), (100,))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "class_corpus_pos.shape, class_corpus_neg.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "H4HGouPtyH4j"
      },
      "outputs": [],
      "source": [
        "for review_index, review in zip(class_corpus_pos.index, class_corpus_pos):\n",
        "    with open(pos_examples_dir / (str(review_index) + \".txt\"), \"w\") as f:\n",
        "        f.write(review)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "5hDND0BdzzLZ"
      },
      "outputs": [],
      "source": [
        "for review_index, review in zip(class_corpus_neg.index, class_corpus_neg):\n",
        "    with open(neg_examples_dir / (str(review_index) + \".txt\"), \"w\") as f:\n",
        "        f.write(review)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gc4uUdswz8L4",
        "outputId": "f81b1f6f-9216-4955-c9f9-fdf17f79a7ed"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "len(list(pos_examples_dir.glob(\"*.txt\"))), len(list(neg_examples_dir.glob(\"*.txt\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBKwQu0l83Wq"
      },
      "source": [
        "To create a text dataset from the corpus, we will need to specify the batch size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "WFfqFWzq0f6A"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RglDsffX0jXA",
        "outputId": "fcd6dc1a-4cb7-4c5c-eb4c-276be76ef8d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 200 files belonging to 2 classes.\n",
            "Using 160 files for training.\n"
          ]
        }
      ],
      "source": [
        "raw_train_ds = tf.keras.utils.text_dataset_from_directory(root_examples_dir,\n",
        "                                                          batch_size=BATCH_SIZE,\n",
        "                                                          validation_split=0.2,\n",
        "                                                          subset='training',\n",
        "                                                          seed=20130810)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZay2ytM1nt3",
        "outputId": "9d8329d4-b973-40fa-ddb8-b0241cbafd6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review b'Maybe it\\xc3\\xa2\\xc2\\x80\\xc2\\x99s just the holiday spirit, but I can\\xc3\\xa2\\xc2\\x80\\xc2\\x99t find it in myself to be particularly frustrated by Holmes and Watson. Admittedly, it\\xc3\\xa2\\xc2\\x80\\xc2\\x99s no Step Brothers, and I can\\xc3\\xa2\\xc2\\x80\\xc2\\x99t help wishing for a more bizarre cut of the film (there\\xc3\\xa2\\xc2\\x80\\xc2\\x99s an undercurrent of weirdness to the whole affair that feels somehow tempered), but if it feels a little elementary compared to the best of Will Ferrell and John C. Reilly\\xc3\\xa2\\xc2\\x80\\xc2\\x99s work (the aforementioned Step Brothers, Talladega Nights: The Ballad of Ricky Bobby, etc.), that\\xc3\\xa2\\xc2\\x80\\xc2\\x99s not necessarily a damning distinction. That Ferrell and Reilly should take on Sherlock Holmes and John Watson, respectively, feels like a logical next step for the comedy duo, as they\\xc3\\xa2\\xc2\\x80\\xc2\\x99ve made something of a specialty out of two-handers featuring Ferrell as the bright star and Reilly as the yearning sidekick. Granted, their particular spin on Arthur Conan Doyle\\xc3\\xa2\\xc2\\x80\\xc2\\x99s most famous creations paints his legendarily brilliant pair as emotionally stunted morons, but the point still stands: There\\xc3\\xa2\\xc2\\x80\\xc2\\x99s an archetype that the story fulfills that\\xc3\\xa2\\xc2\\x80\\xc2\\x99s a perfect match. What makes the film more than a retread, however, is also what makes it so unwieldy. As the detective and doctor work toward foiling an assassination attempt on the Queen (Pam Ferris) by none other than Professor Moriarty (Ralph Fiennes), the situations they find themselves in become increasingly bizarre. Part of it is to do with the fact that cracking the case is secondary to cracking open their hearts, and part of it is a seeming indecision as to exactly what Holmes and Watson is meant to be. Halfway between Step Brothers and Talladega Nights. It\\xc3\\xa2\\xc2\\x80\\xc2\\x99s not quite the absurdist art that Step Brothers is, nor entirely the political commentary of Talladega Nights (a sharper look at the Bush years than this year\\xc3\\xa2\\xc2\\x80\\xc2\\x99s Vice, with all due apologies to Adam McKay); instead, it falls halfway in between, failing to make a completely coherent meal out of either approach. That said, if I had to wager (with the full acknowledgment that such speculation is meaningless), I would say that director Etan Cohen was trying for something more in line with the former, as there are bizarre interludes scattered throughout that suggest a much stranger film. The best word to describe those moments is \\xc3\\xa2\\xc2\\x80\\xc2\\x9cgrotesque,\\xc3\\xa2\\xc2\\x80\\xc2\\x9d as the film\\xc3\\xa2\\xc2\\x80\\xc2\\x99s sense of humor goes beyond the usual scatological jokes \\xc3\\xa2\\xc2\\x80\\xc2\\x94 plus the requisite period comedy jabs at modern culture and technological advancements, and some laughs at the expense of previous iterations of Holmes and Watson \\xc3\\xa2\\xc2\\x80\\xc2\\x94 and ventures into genuinely strange territory. One scene, doubling down on Watson\\xc3\\xa2\\xc2\\x80\\xc2\\x99s status as a sidekick rather than an equal, introduces something called the \\xc3\\xa2\\xc2\\x80\\xc2\\x9ccompanions\\xc3\\xa2\\xc2\\x80\\xc2\\x99 room.\\xc3\\xa2\\xc2\\x80\\xc2\\x9d The roster of characters introduced there is so strange that their split-second appearance, a far cry from the triter set pieces that populate the rest of the film, is deeply jarring. Another gag spins fun out of the effects of poison, while the film\\xc3\\xa2\\xc2\\x80\\xc2\\x99s opening sequence alone involves not only turtle rehabilitation but the coining of the term \\xc3\\xa2\\xc2\\x80\\xc2\\x9cun-crying.\\xc3\\xa2\\xc2\\x80\\xc2\\x9d It\\xc3\\xa2\\xc2\\x80\\xc2\\x99s that weirdness that makes the film\\xc3\\xa2\\xc2\\x80\\xc2\\x99s big emotional payoff \\xc3\\xa2\\xc2\\x80\\xc2\\x94 which, amazingly, comes courtesy of Alan Menken and Glenn Slater \\xc3\\xa2\\xc2\\x80\\xc2\\x94 so fun when it finally kicks in. (The sequence is also shot more inventively than most of the rest of the film, though that\\xc3\\xa2\\xc2\\x80\\xc2\\x99s neither here nor there.) But it doesn\\xc3\\xa2\\xc2\\x80\\xc2\\x99t last long enough, or rather, it comes a little too early in the film out of necessity, as there\\xc3\\xa2\\xc2\\x80\\xc2\\x99s still a Moriarty mystery to be solved. And yes, Fiennes is largely wasted as Moriarty, but what he does get to do is so patently strange that it\\xc3\\xa2\\xc2\\x80\\xc2\\x99s hard to put out of mind. The same goes for Kelly Macdonald as Mrs. Hudson, absolutely swimming in the river of ham, to the point that you almost forget to wonder what she\\xc3\\xa2\\xc2\\x80\\xc2\\x99s doing in such a thankless role; Hugh Laurie as Mycroft Holmes, hinting at the glory of his Blackadder days; Rob Brydon as Lestrade, the requisite straight man who seems to have teleported in from another, better movie; and Steve Coogan as a one-armed tattoo artist, who very nearly steals the whole show with an elastic, easy-breezy performance that, naturally, is cut short a little too soon. The worst parts of Holmes and Watson are mitigated by how weird it is, and by its terrific supporting cast. Rebecca Hall and Lauren Lapkus fare a little better in terms of screen time as Holmes and Watson\\xc3\\xa2\\xc2\\x80\\xc2\\x99s respective love interests, though, ultimately, there\\xc3\\xa2\\xc2\\x80\\xc2\\x99s not enough space in the movie for anybody but the leading men. If, at times, it all feels like an overextended comedic bit, that\\xc3\\xa2\\xc2\\x80\\xc2\\x99s because it is. Holmes and Watson is mostly an excuse for Ferrell and Reilly to fool around with British accents (which transcend being good or bad for how hard they lean into how ridiculous they know they sound \\xc3\\xa2\\xc2\\x80\\xc2\\x94 they still somehow sound better than a certain lamplighter) and point out how wild the past was (the film has multiple references to casual cocaine use). It doesn\\xc3\\xa2\\xc2\\x80\\xc2\\x99t really matter that the case is impossible to solve independently, because it\\xc3\\xa2\\xc2\\x80\\xc2\\x99s not the point of the story, and the film\\xc3\\xa2\\xc2\\x80\\xc2\\x99s relatively thin construction isn\\xc3\\xa2\\xc2\\x80\\xc2\\x99t so severe a crime when the actors can carry most of it off fairly well. Reilly in particular brings a bona fide quality to an otherwise gormless Watson, digging into the sensitivity that served him so well in The Sisters Brothers to what are admittedly diminishing returns. But it still keeps Holmes and Watson from being completely disposable, and the same goes for the whole of the film: Though it doesn\\xc3\\xa2\\xc2\\x80\\xc2\\x99t all quite hold together, there are enough inspired moments \\xc3\\xa2\\xc2\\x80\\xc2\\x94 and enough strangeness lying just below the film\\xc3\\xa2\\xc2\\x80\\xc2\\x99s surface \\xc3\\xa2\\xc2\\x80\\xc2\\x94 that it doesn\\xc3\\xa2\\xc2\\x80\\xc2\\x99t completely fall apart, either.'\n",
            "Label 1\n",
            "Review b\"Taxi\\xc3\\x8acasts Jimmy\\xc3\\x8aFallon as Officer Andy Washburn, a bumbling cop who loses his drivers license after causing thousands of dollars worth of damage while in pursuit of a suspect. En route to a bank robbery, Washburn hails a cab driven by Belle (Queen Latifah) - where the two encounter four criminals that just happen to look like supermodels. Washburn, unable to drive himself to various crime scenes, enlists Belle's help in pursuing the perps - despite the protests of Washburn's Lieutenant (played by Jennifer Esposito). It's clear that\\xc3\\x8aTaxi\\xc3\\x8ais going for a\\xc3\\x8aStakeout\\xc3\\x8asort of vibe, mixing action sequences with wacky hijinks. And though it'd seem all the right ingredients are in place - aside from Fallon, Latifah can be funny when given the right material - there's virtually nothing here that works. Director Tim Story - whose most notable directorial effort thus far is the Ice Cube comedy\\xc3\\x8aBarbershop\\xc3\\x8a- quickly proves to be the absolute wrong choice for this material, infusing the film with incredibly obvious choices and a sense of style that's beyond bland. Example: in the sequence where we first see the models, the soundtrack blares Tone Loc's\\xc3\\x8aWild Thing\\xc3\\x8awhile Story's camera lingers on their bodies (in slow motion, no less!) That sort of thing is indicative of Story's astounding lack of imagination, with the filmmaker, time and again, taking the safest route possible (though, to be fair, this isn't exactly cutting edge material). Unbelievably, it took three people to write this mess - Ben Garant, Thomas Lennon, and Jim Kouf - and their script is peppered with dialogue that seems to exist only to further the tired storyline. Washburn perfectly exemplifies this, as it's impossible to believe a police officer could be this inept and keep his job for more than a few days out of the academy. Rather than use any actual police work to solve the case, Washburn instead relies on tips and suggestions garnered from friends and family (Columbo this guy isn't). And let's not even get into the plausibility of Belle, with her tricked-out taxicab (how on earth could a delivery person afford such a vehicle?) and ability to\\xc3\\x8aalways\\xc3\\x8abe in the exact right place at the right time. Then again, it'd be easy enough to overlook such flaws if the film contained even\\xc3\\x8aone\\xc3\\x8adecent character (which it doesn't). The performances are generally of the sort of caliber one might expect from a movie of this sort, with Fallon delivering a grating, over-the-top performance that's absolutely devoid of charisma. (Latifah, on the other hand, isn't bad and deserves much, much better than this.)\\xc3\\x8aTaxi\\xc3\\x8ais, without a doubt, one of the worst films of the year and a disastrous start to Jimmy Fallon's movie stardom. If he's hoping for a career along the lines of Bill Murray rather than Chevy Chase, he'd better be a little more choosy in the future.\"\n",
            "Label 0\n",
            "Review b'Set in the year 2054, Steven Spielberg\\'s Minority Report looks clammy and bleached-out. The oppressiveness, of course, is intentional, but is it necessary? High-concept science-fiction escapades often try to impose new ways of seeing, but Spielberg seems intent on blistering our optic nerves. Like A.I., Minority Report is a movie in furious conflict with itself. Hope is pitted, rather unsuccessfully, against dystopia -- or is it dyspepsia? The result is one of the glummest and most forbidding thrillers ever.Tom Cruise plays John Anderton, who runs the Justice Department\\'s Precrime unit, which acts on evidence provided by three human \"Pre-cogs\" who float in a liquid suspension chamber and can see visions of future murders. The images they transmit are displayed in haphazard fragments on a giant screen before which Anderton stands like a maestro, sorting out the pictures with sweeping waves of his arms. The garish, fractured visuals may cause some in the audience to experience d\\xc3\\xa9j\\xc3\\xa0 vu: Can it be that the Pre-cogs are channeling the credit sequence from Seven?The Precrime cops, tipped by the trio, swoop down and arrest potential murderers, who are contained in a comatose state in long pneumatic tubes while their misdeeds are played out before their eyes. The moral dilemma is that none of the convicted is actually guilty of anything in the present. But because the Pre-cogs are supposed to be infallible, and because the murder rate in Washington, D.C., for the past six years has dropped to zero, the public is enthusiastic, and there is a political initiative pending to make Precrime go national. Then comes the twist: Anderton is implicated in the murder, within 36 hours, of a man he doesn\\'t even know. Believing himself innocent and set up by his enemies, he goes on the lam. Either he is a murderer-to-be or the Pre-cogs are wrong -- which would call into question everything Anderton stands for as a crime fighter.There are some extraordinary sequences in Minority Report, and they are almost always the ones in which Spielberg lets loose his genius for graphic movement. In one scene, Anderton jumps across lanes of magnetic-levitation cars that speed both horizontally and vertically through the automated cityscape. In the film\\'s best set piece, he submerges himself in a bathtub while mechanical spiders try to identify him by scanning his retina. (The action is so astonishingly deft that the pop of a single air bubble caps the scene.) The futurist vision of a consumer society in Minority Report -- which was written by Scott Frank and Jon Cohen and very loosely based on a longish and dullish 1956 Philip K. Dick story -- is alarmingly plausible. By scanning our eyes, companies will track our whereabouts and personalize the hard sell as never before. Malls will be lined with talking billboards on a first-name basis with us. (This is the rare movie where the plethora of product placements actually serves a legitimate dramatic function.) And yet what\\'s alarming to us may not be quite so scary to Spielberg the corporate-honcho visionary, who simply presents this stuff as a fact of (future) life. His attitude is a wary bemusement.What really spooks him, as in A.I., is that it may no longer be possible to be truly human in such a cyberfuture. Anderton, in addition to his Precrime problems, is also aggrieved by the unsolved kidnapping six years ago of his son. He compulsively plays back holographic home movies of his boy and has a drug habit. All this is meant to humanize Anderton for us, but Tom Cruise doesn\\'t really have much inner life as an actor, and so the effect is muddled. Anderton is supposed to be running away from himself, from his demons, but that\\'s not what comes across. He\\'s running to get away. Cruise has become such a movie-star icon that we\\'re supposed to gasp in awe whenever he gets down and dirty and messes with his iconography. In what amounted to a ghastly piece of reverse narcissism in Vanilla Sky, he spent half the movie sporting a surgically distended face. (It\\'s as if he were saying, \"Look how ugly I make myself and still you watch me.\") In Minority Report, he\\'s such a dynamo that he converts even the process of grieving into a form of aerobics. Cruise carries the movie, but I was relieved to turn to some of the other players, including Colin Farrell as Anderton\\'s nemesis, and Samantha Morton as the most gifted of the Pre-cogs, for more evocative human shadings.Spielberg is still in his Kubrick mode in Minority Report, which is to say that, even more than in A.I., he has split himself off from the verve and emotional sympathy that characterize his best films. I respect his desire to challenge himself and go deeper and darker. It\\'s just that most of what\\'s dark about Minority Report is basically futuro film noir, and what\\'s \"deep\" about it are the standard sci-fi tropes about the individual vs. society and what\\'s real and what isn\\'t, as well as the usual quasi-religious overtones about playing God. Ultimately, the film turns into a soporific anthem for the power to choose one\\'s future, followed by a quick fadeout of family togetherness. Although he can\\'t quite get himself to do it, Spielberg clearly wants to drop the sentimentality from his repertoire: That anthem strikes a hollow chord, and the family idealizations are one-note. Minority Report is, among other things, Spielberg\\'s attempt to extirpate once and for all his inner E.T. Like Cruise, he desires to deface his own image, which is perhaps why this new movie specializes in shots of severed eyeballs -- it\\'s practically a motif. The most visually dynamic director of his generation wants to know what it\\'s like to fly blind. That\\'s what most repels him -- and attracts him. He\\'s virtuosic enough to keep Minority Report aloft, but he\\'s not soaring.'\n",
            "Label 0\n"
          ]
        }
      ],
      "source": [
        "for text_batch, label_batch in raw_train_ds.take(1):\n",
        "  for i in range(3):\n",
        "    print(\"Review\", text_batch.numpy()[i])\n",
        "    print(\"Label\", label_batch.numpy()[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRCQg3_81qxw",
        "outputId": "31255268-8bb7-4f29-e1cc-7beaa140904e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label 0 corresponds to negative\n",
            "Label 1 corresponds to positive\n"
          ]
        }
      ],
      "source": [
        "print(\"Label 0 corresponds to\", raw_train_ds.class_names[0])\n",
        "print(\"Label 1 corresponds to\", raw_train_ds.class_names[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2bJSU6n1yI-",
        "outputId": "2f33237e-563b-4ff1-9d39-afefd8cbd014"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 200 files belonging to 2 classes.\n",
            "Using 40 files for validation.\n"
          ]
        }
      ],
      "source": [
        "raw_val_ds = tf.keras.utils.text_dataset_from_directory(root_examples_dir,\n",
        "                                                        batch_size=BATCH_SIZE,\n",
        "                                                        validation_split=0.2,\n",
        "                                                        subset='validation',\n",
        "                                                        seed=20130810)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARUs0-hS9ZP7"
      },
      "source": [
        "[To speed up training](https://www.tensorflow.org/guide/data_performance), it is beneficial to shuffle, cache and prefetch the samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "BtIbtMD83wAo"
      },
      "outputs": [],
      "source": [
        "train_ds = (raw_train_ds.shuffle(buffer_size=raw_train_ds.cardinality().numpy())\n",
        "                        .cache()\n",
        "                        .prefetch(buffer_size=tf.data.AUTOTUNE))\n",
        "\n",
        "val_ds = (raw_val_ds.shuffle(buffer_size=raw_train_ds.cardinality().numpy())\n",
        "                    .cache()\n",
        "                    .prefetch(buffer_size=tf.data.AUTOTUNE))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEEuEOJp2IH3"
      },
      "source": [
        "## Vectorize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgMs1Z87PKuV"
      },
      "source": [
        "To vectorize the reviews, we use the [TextVectorization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) class. We can cap the vocabulary size to limit the tokens used for modeling to the most frequently appearing ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "zUoXdt0c6m7U"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE = 5000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "grVHtxoa6yBi"
      },
      "outputs": [],
      "source": [
        "encoder = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE,\n",
        "                                            standardize=\"lower_and_strip_punctuation\",\n",
        "                                            pad_to_max_tokens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlmXd2wWA9TV"
      },
      "source": [
        "Note that beyond the default, we could change the `standardize` parameter to use a [custom function](https://www.tensorflow.org/tutorials/keras/text_classification)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "5u7Mfo_g66cx"
      },
      "outputs": [],
      "source": [
        "encoder.adapt(train_ds.map(lambda text, label: text),\n",
        "              batch_size= None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivMatfRh9CsJ",
        "outputId": "102b4986-1703-407f-d759-1cee4f9d6f3d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   1,   57,  290, 1124,   44,    1,   62,   62])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "encoded_example = encoder('encanto we dont talk about bruno no no').numpy()\n",
        "encoded_example[:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ad5kJGMr9cjX",
        "outputId": "0556c140-59c7-4050-d260-feed4f327d2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1340:  threat\n",
            "1:  [UNK]\n",
            "Vocabulary size: 5000\n"
          ]
        }
      ],
      "source": [
        "print(\"1340: \", encoder.get_vocabulary()[1340])\n",
        "print(\"1: \", encoder.get_vocabulary()[1])\n",
        "\n",
        "print(f'Vocabulary size: {len(encoder.get_vocabulary())}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnEqg6S9BwJ1"
      },
      "source": [
        "Note how a special token `[UNK]` is used to identify out of vocabulary (OOV) words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yi6D2EgP34HX"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "EpXqaHXs45bF"
      },
      "outputs": [],
      "source": [
        "NUM_CLASSES = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGemYjmhBN5b"
      },
      "source": [
        "As we have done so far, we make the encoder a part of the model. This ensures that the test data follows the patterns encoded during training (i.e., no data leakage)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Y800usIROEH"
      },
      "source": [
        "The [LSTM cell](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21) augments a vanilla RNN cell by incorporating among other things a gradient flow highway. If the LSTM cell is used as an intermediate layer, then it needs to return a sequence that is used by another downstream LSTM layer as an input (default is to return [the output of the last timestep](https://www.tensorflow.org/guide/keras/rnn))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "j8lPxu3O4HId"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([# encoder generates one hot encoded corpus based on the vocabulary\n",
        "                             encoder,\n",
        "                             # Embedding layer projects this vocabulary down to a smaller dimension\n",
        "                             tf.keras.layers.Embedding(len(encoder.get_vocabulary()),\n",
        "                                                       64,\n",
        "                                                       mask_zero=True),\n",
        "                             # The first LSTM layer uses the outputs from the embedding layer\n",
        "                             # as the input sequence\n",
        "                             tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,\n",
        "                                                                                return_sequences=True,\n",
        "                                                                                dropout=0.3)),\n",
        "                             # The second LSTM layer uses the outputs from the first LSTM\n",
        "                             # layer as the input sequence\n",
        "                             tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32,\n",
        "                                                                                dropout=0.3)),\n",
        "                             # The Dense layer uses outputs from the last time step of\n",
        "                             # the LSTM layer\n",
        "                             tf.keras.layers.Dense(64, activation='relu'),\n",
        "                             # The final layer outputs the\n",
        "                             tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aYY42luyNfc"
      },
      "source": [
        "Text is a special kind of a time-series where the context around the word in both directions encodes meaningful information. Hence, when working with text we use a [Bidirectional layer](https://www.tensorflow.org/guide/keras/rnn#bidirectional_rnns). The Bidirectional layer copies the LSTM layer and processes inputs in opposing orders as they run through the network. It then concatenates the forward layer output and backward layer output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FJbevzO4-vM",
        "outputId": "d32f3e85-cfb8-4392-f05e-607a782e19fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization (TextVe  (None, None)              0         \n",
            " ctorization)                                                    \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 64)          320000    \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, None, 128)         66048     \n",
            " al)                                                             \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirecti  (None, 64)                41216     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                4160      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 130       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 431554 (1.65 MB)\n",
            "Trainable params: 431554 (1.65 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "uRvFRit_5MO8"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer= tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfZF7Biu5R8Y",
        "outputId": "550ab432-1295-441e-ab38-1ed2ee8612f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "54/54 [==============================] - 48s 438ms/step - loss: 0.6937 - accuracy: 0.4187 - val_loss: 0.6926 - val_accuracy: 0.6000\n",
            "Epoch 2/200\n",
            "54/54 [==============================] - 7s 127ms/step - loss: 0.6933 - accuracy: 0.4750 - val_loss: 0.6928 - val_accuracy: 0.6250\n",
            "Epoch 3/200\n",
            "54/54 [==============================] - 9s 162ms/step - loss: 0.6931 - accuracy: 0.4812 - val_loss: 0.6930 - val_accuracy: 0.5500\n",
            "Epoch 4/200\n",
            "54/54 [==============================] - 7s 132ms/step - loss: 0.6933 - accuracy: 0.4688 - val_loss: 0.6932 - val_accuracy: 0.5250\n",
            "Epoch 5/200\n",
            "54/54 [==============================] - 9s 163ms/step - loss: 0.6928 - accuracy: 0.5625 - val_loss: 0.6934 - val_accuracy: 0.4250\n",
            "Epoch 6/200\n",
            "54/54 [==============================] - 8s 145ms/step - loss: 0.6926 - accuracy: 0.5938 - val_loss: 0.6936 - val_accuracy: 0.4500\n",
            "Epoch 7/200\n",
            "54/54 [==============================] - 7s 130ms/step - loss: 0.6926 - accuracy: 0.5875 - val_loss: 0.6939 - val_accuracy: 0.3750\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(train_ds,\n",
        "                    epochs=200,\n",
        "                    validation_data=val_ds,\n",
        "                    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
        "                                                                patience=5)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtQ74k63KGT0"
      },
      "source": [
        "# References\n",
        "\n",
        "1. [Illustrated Guide to LSTM](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)\n",
        "2. [Understanding LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}