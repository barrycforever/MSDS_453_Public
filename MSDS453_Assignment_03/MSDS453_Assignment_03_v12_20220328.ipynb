{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/djp840/MSDS_453_Public/blob/main/images/NorthwesternHeader.png?raw=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSDS453 - Research Assignment 03 - Ontology Plus Context and Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the ontology that you developed in week 6 and consider how your ten chosen documents for this quarter's class corpus map to that ontology.\n",
    "\n",
    "Protégé is a tool that takes inputs for a human-created Ontology and creates a visualization. \n",
    "\n",
    "Use Python algorithms to generate Knowledge Graphs from your documents.\n",
    "\n",
    "TensorFlow Bidirectional RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "from spacy.matcher import Matcher \n",
    "from spacy.tokens import Span \n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "tf.random.set_seed(2022)\n",
    "\n",
    "from typing import List, Callable, Dict, Tuple, Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Required Installation: en_core_web_lg</b><br>\"python -m spacy download en_core_web_lg\"<br><br>English pipeline optimized for CPU. Components: tok2vec, tagger, parser, senter, ner, attribute_ruler, lemmatizer<br>\n",
    "    <b>https://spacy.io/models/en#en_core_web_lg </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mount Google Drive to Colab Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this once, they will be downloaded.\n",
    "nltk.download('stopwords',quiet=True)\n",
    "nltk.download('wordnet',quiet=True)\n",
    "nltk.download('punkt',quiet=True)\n",
    "nltk.download('omw-1.4',quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Gensim</b> is a Python library for topic modelling, document indexing and similarity retrieval with large corpora. Target audience is the natural language processing (NLP) and information retrieval (IR) community <br><br>\n",
    "    <b>https://pypi.org/project/gensim/ </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "pkg_resources.require(\"gensim<=3.8.3\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Genism Version: \", gensim.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Suppress warning messages</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_movie_descriptor(data: pd.DataFrame, corpus_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Adds \"Movie Description\" to the supplied dataframe, in the form {Genre}_{P|N}_{Movie Title}_{DocID}\n",
    "    \"\"\"\n",
    "    review = np.where(corpus_df['Review Type (pos or neg)'] == 'Positive', 'P', 'N')\n",
    "    data['Descriptor'] = corpus_df['Genre of Movie'] + '_' + corpus_df['Movie Title'] + '_' + review + '_' + corpus_df['Doc_ID'].astype(str)\n",
    "    \n",
    "def get_corpus_df(path: str) -> pd.DataFrame:\n",
    "    data = pd.read_csv(path)\n",
    "    add_movie_descriptor(data, data)\n",
    "    sorted_data = data.sort_values(['Descriptor'])\n",
    "    indexed_data = sorted_data.set_index(['Doc_ID'])\n",
    "    indexed_data['Doc_ID'] = indexed_data.index\n",
    "    return indexed_data\n",
    "\n",
    "def remove_punctuation(text: str) -> str:\n",
    "    return re.sub('[^a-zA-Z]', '', str(text))\n",
    "\n",
    "def remove_tags(text: str) -> str:    \n",
    "    return re.sub('&lt;/?.*?&gt;', '', text)\n",
    "\n",
    "def remove_special_chars_and_digits(text: str) -> str:\n",
    "    return re.sub('(\\\\d|\\\\W)+', '', text)\n",
    "\n",
    "def get_sentences(text: str) -> List[str]:\n",
    "    return [str(x) for x in nlp(text).sents]\n",
    "\n",
    "def get_coref_resolved_sentences(text: str) -> List[str]:\n",
    "    return [str(x) for x in nlp(text).sents]\n",
    "\n",
    "def get_lemmas(text: str, stopwords: Set[str]) -> List[str]:\n",
    "    initial = [remove_tags(remove_special_chars_and_digits(remove_punctuation(x.lemma_.lower()))) for x in nlp(text)]\n",
    "    return [x for x in initial if x not in stopwords]\n",
    "\n",
    "def lemmatize_sentence(text: str, stopwords: Set[str]) -> str:\n",
    "    return ' '.join(get_lemmas(text, stopwords))\n",
    "\n",
    "def clean_doc(doc): \n",
    "    #doc = remove_punctuation(doc)\n",
    "    doc= ' '.join(remove_stop_words(doc))\n",
    "    doc = apply_lemmatization(doc)\n",
    "    return doc\n",
    "\n",
    "def remove_stop_words(in_text):\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    word_tokens = word_tokenize(in_text)  \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "    return filtered_sentence\n",
    "\n",
    "def apply_lemmatization(in_text):\n",
    "    # Lemmatization\n",
    "    lem = WordNetLemmatizer()\n",
    "    word_list = nltk.word_tokenize(in_text)\n",
    "    output = ' '.join([lem.lemmatize(w) for w in word_list])\n",
    "    return output\n",
    "\n",
    "def counter_word(text):\n",
    "  count=Counter()\n",
    "  for i in text.values:\n",
    "    for word in i.split():\n",
    "      count[word]=+1\n",
    "  return count\n",
    "\n",
    "def map_edges(map_to: str, map_from: Set[str], df: pd.DataFrame):\n",
    "    print(f'Before mapping {\", \".join(map_from)} -> {map_to}: {sum(df.edge == map_to)}')\n",
    "    df['edge'] = np.where(kg_df.edge.isin(map_from), map_to, kg_df.edge)\n",
    "    print(f'After mapping {\", \".join(map_from)} -> {map_to}: {sum(df.edge == map_to)}')\n",
    "    \n",
    "def map_sources_and_targets(map_to: str, map_from: Set[str], df: pd.DataFrame):\n",
    "    before = sum(df.source == map_to) + sum(df.target == map_to)\n",
    "    print(f'Before mapping {\", \".join(map_from)} -> {map_to}: {before}')\n",
    "    \n",
    "    df['source'] = np.where(kg_df.source.isin(map_from), map_to, kg_df.source)\n",
    "    df['target'] = np.where(kg_df.target.isin(map_from), map_to, kg_df.target)\n",
    "    \n",
    "    after = sum(df.source == map_to) + sum(df.target == map_to)\n",
    "    print(f'After mapping {\", \".join(map_from)} -> {map_to}: {after}')\n",
    "    \n",
    "def get_neighborhood(sources: Set[str], edge_types: Set[str], depth: int, df: pd.DataFrame) -> pd.DataFrame:\n",
    "    output = []\n",
    "    \n",
    "    for d in range(depth):\n",
    "        if edge_types is not None:\n",
    "            rows = df[(df.edge.isin(edge_types)) & ((df.source.isin(sources)) | (df.target.isin(sources)))].copy()\n",
    "        else:\n",
    "            rows = df[(df.source.isin(sources)) | (df.target.isin(sources))].copy()\n",
    "            \n",
    "        output.append(rows)\n",
    "        sources = set(rows.target).union(set(rows.source))\n",
    "        \n",
    "    return pd.concat(output).drop_duplicates()\n",
    "\n",
    "def find_sources_and_targets_with_patterns(patterns: List[str], df: pd.DataFrame):\n",
    "    mask = np.zeros(kg_df.shape[0])\n",
    "    for pattern in patterns:\n",
    "        mask = mask | (df.source.str.contains(pattern)) | (df.target.str.contains(pattern))\n",
    "        \n",
    "    return df[mask]\n",
    "\n",
    "def plot_graph(df: pd.DataFrame, show_edges: bool = False, figsize: Tuple[int, int] = (12, 12), use_circular: bool=True):\n",
    "    graph = nx.from_pandas_edgelist(df, \"source\", \"target\", edge_attr='edge', create_using=nx.MultiDiGraph())\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    if use_circular:\n",
    "        pos = nx.circular_layout(graph)\n",
    "    else:\n",
    "        pos = nx.kamada_kawai_layout(graph)\n",
    "        \n",
    "    nx.draw(graph, with_labels=True, node_color='skyblue', edge_cmap=plt.cm.Blues, pos=pos)\n",
    "    if show_edges:\n",
    "        nx.draw_networkx_edge_labels(graph, pos=pos, font_size=8)\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "def get_top_sources_and_targets(df: pd.DataFrame, top: int = 10):\n",
    "    return (Counter(df.source) + Counter(df.target)).most_common(top)\n",
    "\n",
    "def get_top_edges(df: pd.DataFrame, top: int = 10):\n",
    "    return Counter(df.edge).most_common(top)\n",
    "\n",
    "def get_dataset_partitions_pd(df, train_split=0.8, val_split=0.10, test_split=0.10):\n",
    "       # Specify seed to always have the same split distribution between runs\n",
    "    df_sample = df.sample(frac=1, random_state=12)\n",
    "    indices_or_sections = [int(.8*len(df)), int(.9*len(df))]\n",
    "    train_ds, val_ds, test_ds = np.split(df_sample, indices_or_sections)\n",
    "    return train_ds, val_ds, test_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(sent):\n",
    "  ## chunk 1\n",
    "  ent1 = \"\"\n",
    "  ent2 = \"\"\n",
    "\n",
    "  prv_tok_dep = \"\"    # dependency tag of previous token in the sentence\n",
    "  prv_tok_text = \"\"   # previous token in the sentence\n",
    "\n",
    "  prefix = \"\"\n",
    "  modifier = \"\"\n",
    "\n",
    "  for tok in nlp(sent):\n",
    "    ## chunk 2\n",
    "    # if token is a punctuation mark then move on to the next token\n",
    "    if tok.dep_ != \"punct\":\n",
    "      # check: token is a compound word or not\n",
    "      if tok.dep_ == \"compound\":\n",
    "        prefix = tok.text\n",
    "        # if the previous word was also a 'compound' then add the current word to it\n",
    "        if prv_tok_dep == \"compound\":\n",
    "          prefix = prv_tok_text + \" \"+ tok.text\n",
    "      \n",
    "      # check: token is a modifier or not\n",
    "      if tok.dep_.endswith(\"mod\") == True:\n",
    "        modifier = tok.text\n",
    "        # if the previous word was also a 'compound' then add the current word to it\n",
    "        if prv_tok_dep == \"compound\":\n",
    "          modifier = prv_tok_text + \" \"+ tok.text\n",
    "      \n",
    "      ## chunk 3\n",
    "      if tok.dep_.find(\"subj\") == True:\n",
    "        ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n",
    "        prefix = \"\"\n",
    "        modifier = \"\"\n",
    "        prv_tok_dep = \"\"\n",
    "        prv_tok_text = \"\"      \n",
    "\n",
    "      ## chunk 4\n",
    "      if tok.dep_.find(\"obj\") == True:\n",
    "        ent2 = modifier +\" \"+ prefix +\" \"+ tok.text\n",
    "        \n",
    "      ## chunk 5  \n",
    "      # update variables\n",
    "      prv_tok_dep = tok.dep_\n",
    "      prv_tok_text = tok.text\n",
    "\n",
    "  return [ent1.strip(), ent2.strip()]\n",
    "\n",
    "def get_relation(sent):\n",
    "    try:\n",
    "        doc = nlp(sent)\n",
    "        \n",
    "        # Matcher class object \n",
    "        matcher = Matcher(nlp.vocab)\n",
    "\n",
    "        #define the pattern \n",
    "        pattern = [{'DEP':'ROOT'}, \n",
    "                {'DEP':'prep','OP':\"?\"},\n",
    "                {'DEP':'agent','OP':\"?\"},  \n",
    "                {'POS':'ADJ','OP':\"?\"}] \n",
    "        matcher.add(\"matching_1\", [pattern]) \n",
    "        matches = matcher(doc)\n",
    "        k = len(matches) - 1\n",
    "        span = doc[matches[k][1]:matches[k][2]] \n",
    "        \n",
    "        return(span.text)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "def get_subject_verb_object(sent):\n",
    "  ent1 = \"\"\n",
    "  ent2 = \"\"\n",
    "  root = \"\"\n",
    "\n",
    "  for tok in nlp(sent):\n",
    "      if tok.dep_ == 'ROOT':\n",
    "        root = tok.text\n",
    "      elif tok.dep_ == \"nsubj\":\n",
    "        ent1 = tok.text\n",
    "      elif tok.dep_ == \"dobj\":\n",
    "        ent2 = tok.text\n",
    "\n",
    "      if ent1 != '' and ent2 != '' and root != '':\n",
    "        break\n",
    "\n",
    "  return [ent1, root, ent2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix_labeled(y_true, y_pred, CLASSES_LIST):\n",
    "    mtx = confusion_matrix(y_true, y_pred)\n",
    "    # define classes \n",
    "    classes = CLASSES_LIST\n",
    "    temp_df = pd.DataFrame(data=mtx,columns=classes)\n",
    "    temp_df.index = classes\n",
    "    fig, ax = plt.subplots(figsize=(10,10))\n",
    "    sns.heatmap(temp_df, annot=True, fmt='d', linewidths=.75,  cbar=False, ax=ax,cmap='Blues',linecolor='white')\n",
    "    #  square=True,\n",
    "    plt.ylabel('true label')\n",
    "    plt.xlabel('predicted label')\n",
    "    \n",
    "def plot_graphs(history, metric):\n",
    "  plt.plot(history.history[metric])\n",
    "  plt.plot(history.history['val_'+metric], '')\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(metric)\n",
    "  plt.legend([metric, 'val_'+metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Class Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_PATH = './data/MSDS453_QA_TEST.csv'\n",
    "corpus_df = get_corpus_df(CORPUS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize sentences\n",
    "corpus_df['raw_sentences'] = corpus_df.Text.apply(get_sentences)\n",
    "corpus_df.head(3).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize words\n",
    "default_stopwords = set(nltk.corpus.stopwords.words('english')).union(set(nlp.Defaults.stop_words)).union({' ', ''})\n",
    "corpus_df['lemmas'] = corpus_df.Text.apply(lambda x: get_lemmas(x, default_stopwords))\n",
    "corpus_df.lemmas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reassemble lemmas\n",
    "corpus_df['lemmas_joined'] = corpus_df.lemmas.apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a set of additional words to remove by DF\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
    "transformed_documents = vectorizer.fit_transform(corpus_df.lemmas_joined)\n",
    "doc_term_matrix = transformed_documents.todense()\n",
    "doc_term_df = pd.DataFrame(doc_term_matrix, \n",
    "                           columns=vectorizer.get_feature_names_out(), \n",
    "                           index=corpus_df.Descriptor)\n",
    "print(f'Vocabulary size: {doc_term_df.shape[1]}')\n",
    "all_words = set(doc_term_df.columns)\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 1), min_df=5, max_df=.8)\n",
    "transformed_documents = vectorizer.fit_transform(corpus_df.lemmas_joined)\n",
    "doc_term_matrix = transformed_documents.todense()\n",
    "doc_term_df = pd.DataFrame(doc_term_matrix, \n",
    "                           columns=vectorizer.get_feature_names_out(), \n",
    "                           index=corpus_df.Descriptor)\n",
    "print(f'Vocabulary size: {doc_term_df.shape[1]}')\n",
    "vocabulary = set(doc_term_df.columns)\n",
    "\n",
    "words_to_remove = default_stopwords.union(all_words - vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(doc_term_df.sum(axis=0).T, range=(0, 200))\n",
    "plt.xlabel('Number of Occurrences')\n",
    "plt.ylabel('Number of Terms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-tokenize words, recreate joined documents\n",
    "corpus_df['lemmas'] = corpus_df.Text.apply(lambda x: get_lemmas(x, words_to_remove))\n",
    "corpus_df['lemmas_joined'] = corpus_df.lemmas.apply(lambda x: ' '.join(x))\n",
    "corpus_df.lemmas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get lemmatized and filtered sentences\n",
    "corpus_df['sentences_lemmatized'] = corpus_df.raw_sentences.apply(lambda x: [lemmatize_sentence(s, words_to_remove) for s in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Documents By Movie Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus_df['Movie Title'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "movie_df = corpus_df[corpus_df['Movie Title'] == 'Groundhog_Day'].copy()\n",
    "movie_df.head(3).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge Graph Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_text_sentences = [y for x in movie_df.raw_sentences for y in x]\n",
    "example_sentence = nlp(corpus_text_sentences[1])\n",
    "corpus_text_sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_pairs = [get_entities(x) for x in tqdm(corpus_text_sentences)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataFrame (Source, Target, Edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations = [get_relation(x) for x in corpus_text_sentences]   \n",
    "#extract subject and object\n",
    "source = [i[0] for i in entity_pairs]\n",
    "target = [i[1] for i in entity_pairs]\n",
    "exp1_df = kg_df = pd.DataFrame({'source': source, 'target': target, 'edge': relations})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knowledge Graph Preprocessing (Lowercase, Remove Empty Spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move everything to lower case\n",
    "kg_df.source = kg_df.source.str.lower()\n",
    "kg_df.target = kg_df.target.str.lower()\n",
    "kg_df.edge = kg_df.edge.str.lower()\n",
    "\n",
    "# Filter out empties\n",
    "kg_df = kg_df[kg_df.source != '']\n",
    "kg_df = kg_df[kg_df.target != '']\n",
    "kg_df = kg_df[kg_df.edge != ''].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg_df.head(9).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus_text_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graph(kg_df, use_circular=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network (RNN) Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/djp840/MSDS_453_Public/blob/main/images/BidirectionalRNN.png?raw=true\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>tf.keras.layers.Bidirectional</b><br>\n",
    "    https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "datafull=corpus_df.copy() \n",
    "datafull.reset_index(drop=True, inplace=True)\n",
    "datafull.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafull['Text'] = datafull['Text'].apply(lambda x :clean_doc(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datafull[['Text','Genre of Movie']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Genre of Movie'] = data['Genre of Movie'].astype(\"category\")\n",
    "data['Genre of Movie code'] = data['Genre of Movie'].cat.codes\n",
    "data['Genre of Movie code'].tail().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, targets = data['Text'], data['Genre of Movie code']\n",
    " \n",
    "train_features, test_features, train_targets, test_targets = train_test_split( features, targets , test_size=0.20, random_state=42, shuffle = True,\n",
    "        stratify=targets)\n",
    "data[[\"Genre of Movie\",\"Genre of Movie code\"]].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Datasets for Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainds, valds, testds = get_dataset_partitions_pd(data[['Text','Genre of Movie code']])\n",
    "trainds.shape, valds.shape, testds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert DataFrame to TensorFlow DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train X & y\n",
    "train_text_ds_raw = tf.data.Dataset.from_tensor_slices(\n",
    "            tf.cast(trainds['Text'].values, tf.string)\n",
    ") \n",
    "train_cat_ds_raw = tf.data.Dataset.from_tensor_slices(\n",
    "            tf.cast(trainds['Genre of Movie code'].values, tf.int64),\n",
    "  ) \n",
    "# test X & y\n",
    "test_text_ds_raw = tf.data.Dataset.from_tensor_slices(\n",
    "            tf.cast(testds['Text'].values, tf.string)\n",
    ") \n",
    "test_cat_ds_raw = tf.data.Dataset.from_tensor_slices(\n",
    "            tf.cast(testds['Genre of Movie code'].values, tf.int64),\n",
    ")\n",
    "#val X & Y\n",
    "val_text_ds_raw = tf.data.Dataset.from_tensor_slices(\n",
    "            tf.cast(valds['Text'].values, tf.string)\n",
    ") \n",
    "val_cat_ds_raw = tf.data.Dataset.from_tensor_slices(\n",
    "            tf.cast(valds['Genre of Movie code'].values, tf.int64),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataSets (X=Preprocessed Text, Y=Encoded Categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.zip(\n",
    "    (\n",
    "            train_text_ds_raw,\n",
    "            train_cat_ds_raw\n",
    "     )\n",
    ")\n",
    "test_ds = tf.data.Dataset.zip(\n",
    "    (\n",
    "            test_text_ds_raw,\n",
    "            test_cat_ds_raw\n",
    "     )\n",
    ")\n",
    "val_ds = tf.data.Dataset.zip(\n",
    "    (\n",
    "            val_text_ds_raw,\n",
    "            val_cat_ds_raw\n",
    "     )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data Pipelines (Batching, Shuffling, and Optimizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "buffer_size=train_ds.cardinality().numpy()\n",
    "\n",
    "train_ds = train_ds.shuffle(buffer_size=buffer_size)\\\n",
    "                   .batch(batch_size=batch_size,drop_remainder=True)\\\n",
    "                   .cache()\\\n",
    "                   .prefetch(AUTOTUNE)\n",
    "\n",
    "test_ds = test_ds.shuffle(buffer_size=buffer_size)\\\n",
    "                   .batch(batch_size=batch_size,drop_remainder=True)\\\n",
    "                   .cache()\\\n",
    "                   .prefetch(AUTOTUNE)\n",
    "\n",
    "\n",
    "val_ds = val_ds.shuffle(buffer_size=buffer_size)\\\n",
    "                   .batch(batch_size=batch_size,drop_remainder=True)\\\n",
    "                   .cache()\\\n",
    "                   .prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Text Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw text loaded by `tfds` needs to be processed before it can be used in a model. The simplest way to process text for training is using the `experimental.preprocessing.TextVectorization` layer. This layer has many capabilities, but this tutorial sticks to the default behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the layer, and pass the dataset's text to the layer's `.adapt` method:\n",
    "The processing of each sample contains the following steps:\n",
    "\n",
    "    standardize each sample (usually lowercasing + punctuation stripping)\n",
    "    split each sample into substrings (usually words)\n",
    "    recombine substrings into tokens (usually ngrams)\n",
    "    index tokens (associate a unique int value with each token)\n",
    "    transform each sample using this index, either into a vector of ints or a dense float vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_features.to_numpy()\n",
    "y_train = train_targets.to_numpy()\n",
    "x_test = test_features.to_numpy()\n",
    "y_test = test_targets.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE=5000\n",
    "encoder = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE, standardize=\"lower_and_strip_punctuation\", pad_to_max_tokens= True)\n",
    "encoder.adapt(train_ds.map(lambda text, label: text), batch_size= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.array(encoder.get_vocabulary())\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = np.array(encoder.get_vocabulary())\n",
    "vocab[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Encoded Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_example = encoder('Encanto Nobody Talks About Buno').numpy()\n",
    "encoded_example[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(encoder.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder(data['Text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for example, label in train_ds.take(1):\n",
    "  print('texts: ', example.numpy()[:1])\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build RNN Sequential Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Module: tf.keras.layers</b><br>\n",
    "    https://www.tensorflow.org/api_docs/python/tf/keras/layers\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=5\n",
    "model=tf.keras.Sequential([encoder\n",
    "   ,tf.keras.layers.Embedding(len(encoder.get_vocabulary()), 64, mask_zero=True)\n",
    "   ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True, dropout=0.3))\n",
    "   ,tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, dropout=0.3))\n",
    "   ,tf.keras.layers.Dense(64, activation='relu')\n",
    "   ,tf.keras.layers.Dense(num_classes,activation='softmax')  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer= tf.keras.optimizers.Adam( )\n",
    "              ,loss=tf.keras.losses.SparseCategoricalCrossentropy() \n",
    "              ,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history = model.fit(train_ds\n",
    "         ,epochs=200\n",
    "         ,validation_data=val_ds\n",
    "         ,validation_steps=3\n",
    "         ,callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(val_ds)\n",
    "\n",
    "print('Test Loss: {}'.format(test_loss))\n",
    "print('Test Accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Performance Metrics - Multi-Layer Bidirectional RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df=pd.DataFrame(history_dict)\n",
    "history_df.tail().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = history.history['loss']\n",
    "accs = history.history['accuracy']\n",
    "val_losses = history.history['val_loss']\n",
    "val_accs = history.history['val_accuracy']\n",
    "epochs = len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "for i, metrics in enumerate(zip([losses, accs], [val_losses, val_accs], ['Loss', 'Accuracy'])):\n",
    "    plt.subplot(1, 2, i + 1)\n",
    "    plt.plot(range(epochs), metrics[0], label='Training {}'.format(metrics[2]))\n",
    "    plt.plot(range(epochs), metrics[1], label='Validation {}'.format(metrics[2]))\n",
    "    plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix Post Model Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds2 = model.predict(test_ds)\n",
    "y_pred2 = np.argmax(preds2, axis=1)\n",
    "y2 = np.concatenate([y for x, y in test_ds], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES_LIST = ['Action','Comedy','Horror','Sci Fi','Drama']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix_labeled(y2,y_pred2, CLASSES_LIST=CLASSES_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = sns.light_palette((260, 75, 60), input=\"husl\", as_cmap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(preds2[0:15]\n",
    "                  ,columns = CLASSES_LIST).T\n",
    "df2.style.format(\"{:.2%}\").background_gradient(cmap=cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
