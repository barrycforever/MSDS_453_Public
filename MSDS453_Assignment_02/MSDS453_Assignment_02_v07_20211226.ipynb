{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/djp840/MSDS_453_Public/blob/main/images/NorthwesternHeader.png?raw=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSDS453 - Research Assignment 02 - Classification and Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering methods compute a similarity (or closeness) measure, such as Euclidean distance, in order to decide whether two documents are ‘similar’ or not.  \n",
    "\n",
    "Use the entire class corpus and do sentiment analysis for the positive and negative reviews.\n",
    "\n",
    "Topic modeling is another way to group ‘similar’ documents into ‘clusters’. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle \n",
    "import re,string\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD, LatentDirichletAllocation\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gensim.models import Word2Vec,LdaMulticore, TfidfModel, CoherenceModel\n",
    "from gensim import corpora\n",
    "from gensim import similarities\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim import corpora\n",
    "from gensim.models import LsiModel,LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mount Google Drive to Colab Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions Create TFIDF and Doc2vec Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_doc(doc): \n",
    "    #split document into individual words\n",
    "    tokens=doc.split()\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 4]\n",
    "    #lowercase all words\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]         \n",
    "    # word stemming    \n",
    "    # ps=PorterStemmer()\n",
    "    # tokens=[ps.stem(word) for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "def final_processed_text(doc):\n",
    "    #this is a function to join the processed text back\n",
    "    ' '.join(doc)\n",
    "    return doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(corpus, titles, ngram_range = (1,1)):\n",
    "    #this is a function to created the tfidf matrix\n",
    "    Tfidf=TfidfVectorizer(ngram_range=(1,1))\n",
    "\n",
    "    #fit the vectorizer using final processed documents.  The vectorizer requires the \n",
    "    #stiched back together document.\n",
    "\n",
    "    TFIDF_matrix=Tfidf.fit_transform(corpus)     \n",
    "\n",
    "    #creating datafram from TFIDF Matrix\n",
    "    words = Tfidf.get_feature_names()\n",
    "    matrix=pd.DataFrame(TFIDF_matrix.toarray(), columns=Tfidf.get_feature_names(), index=titles)\n",
    "    return matrix, words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Semantic Analysis, Allocation, Word2Vec Matrix \n",
    "1. Latent Semantic Analysis Example\n",
    "2. Latent Dirichlet Allocation\n",
    "3. Word2Vec Matrix creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Semantic Analysis Using Gensim's Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gensim_lsa_model(doc_clean,number_of_topics,words):\n",
    "\n",
    "    # Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n",
    "    dictionary = corpora.Dictionary(doc_clean)\n",
    "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "    # generate LSA model\n",
    "    lsamodel = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)  # train model\n",
    "    print(lsamodel.print_topics(num_topics=number_of_topics, num_words=words))\n",
    "    index = similarities.MatrixSimilarity(lsamodel[doc_term_matrix])\n",
    "\n",
    "    return lsamodel,dictionary,index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Semantic Analysis Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsa(tfidf_matrix, terms, n_components = 10):\n",
    "    #this is a function to execute lsa.  inputs to the function include the tfidf matrix and\n",
    "    #the desired number of components.\n",
    "    \n",
    "    LSA = TruncatedSVD(n_components=10)\n",
    "\n",
    "    LSA.fit(tfidf_matrix)\n",
    "\n",
    "\n",
    "    for i, comp in enumerate(LSA.components_):\n",
    "        terms_comp = zip(terms, comp)\n",
    "        sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:7]\n",
    "        print(\"Topic \"+str(i)+\": \")\n",
    "        for t in sorted_terms:\n",
    "            print(t[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda(tfidf_matrix, terms, topics = 3, num_words = 10):\n",
    "    #this is a function to perform lda on the tfidf matrix.  function varibales include:\n",
    "    #tfidf matrix, desired number of topic, and number of words per topic.\n",
    "\n",
    "    topics = 3\n",
    "    num_words = 10\n",
    "    lda = LatentDirichletAllocation(n_components=topics).fit(tfidf_matrix)\n",
    "\n",
    "    topic_dict = {}\n",
    "    for topic_num, topic in enumerate(lda.components_):\n",
    "        topic_dict[topic_num] = \" \".join([terms[i]for i in topic.argsort()[:-num_words - 1:-1]])\n",
    "\n",
    "    print(topic_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Matrix Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec(processed_text, size = 100):\n",
    "    #This is a function to generate the word2vec matrix. Input parameters include the \n",
    "    #tokenized text and matrix size\n",
    "    \n",
    "    #word to vec\n",
    "    model_w2v = Word2Vec(processed_text, size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "    #join all processed DSI words into single list\n",
    "    processed_text_w2v=[]\n",
    "    for i in processed_text:\n",
    "        for k in i:\n",
    "            processed_text_w2v.append(k)\n",
    "\n",
    "    #obtian all the unique words from DSI\n",
    "    w2v_words=list(set(processed_text_w2v))\n",
    "\n",
    "    #can also use the get_feature_names() from TFIDF to get the list of words\n",
    "    #w2v_words=Tfidf.get_feature_names()\n",
    "\n",
    "    #empty dictionary to store words with vectors\n",
    "    w2v_vectors={}\n",
    "\n",
    "    #for loop to obtain weights for each word\n",
    "    for i in w2v_words:\n",
    "        temp_vec=model_w2v.wv[i]\n",
    "        w2v_vectors[i]=temp_vec\n",
    "\n",
    "    #create a final dataframe to view word vectors\n",
    "    w2v_df=pd.DataFrame(w2v_vectors).transpose()\n",
    "    print(w2v_df)\n",
    "    return w2v_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section of Code Prepares the Required Variables for Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in class corpus csv into python\n",
    "data=pd.read_csv(r'change me /path/to/source/data/Class_Corpus_v2.csv')\n",
    "\n",
    "#adding two columns to the dataframe to store the processed text and tokenized text\n",
    "data['processed_text'] = data['Text'].apply(lambda x: clean_doc(x))\n",
    "\n",
    "#creating final processed text variables for matrix creation\n",
    "final_processed_text = [' '.join(x) for x in data['processed_text'].tolist()]\n",
    "titles = data['DSI_Title'].tolist()\n",
    "processed_text = data['processed_text'].tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the LSA based on user defined number of topics and words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lsa(number_of_topics, words):\n",
    "\n",
    "    ######################################\n",
    "    #Function to plot the LSA based on user defined number of topics and words\n",
    "    #EXPERIMENT WITH THESE PARAMETERS topics and words\n",
    "\n",
    "    #####################################\n",
    "    model,dictionary,index=create_gensim_lsa_model(processed_text,number_of_topics,words)\n",
    "\n",
    "    for doc in processed_text:\n",
    "        vec_bow = dictionary.doc2bow(doc)\n",
    "        vec_lsi = model[vec_bow]  # convert the query to LSI space\n",
    "        sims = index[vec_lsi] # perform a similarity query against the corpus\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(30, 10))\n",
    "    cax = ax.matshow(index, interpolation='nearest')\n",
    "    ax.grid(True)\n",
    "    plt.xticks(range(len(processed_text)), titles, rotation=90);\n",
    "    plt.yticks(range(len(processed_text)), titles);\n",
    "    fig.colorbar(cax)\n",
    "    plt.show()\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting 6 topics and 10 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_6topic_10words = plot_lsa(6, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting 2 topics and 10 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2topic_10words = plot_lsa(2, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting 20 topics and 10 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_20topic_10words =plot_lsa(20, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare LSA Model Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [6, 2, 10]\n",
    "coherence_values = []\n",
    "for t in topics:\n",
    "    lsamodel,dictionary,index = create_gensim_lsa_model(processed_text,t,10)\n",
    "\n",
    "\n",
    "    coherence_model_lsa = CoherenceModel(model=lsamodel, dictionary=dictionary, texts=processed_text, coherence='c_v')\n",
    "    coherence_lsa = coherence_model_lsa.get_coherence()\n",
    "    coherence_values.append(coherence_lsa)\n",
    "\n",
    "    \n",
    "coherence ={'6 topic 10 words':coherence_values[0],\n",
    "           '2 topic 10 words': coherence_values[1],\n",
    "           '20 topic 10 words':coherence_values[2]}   \n",
    "\n",
    "print(coherence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation Using Gensim's implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gensim_lda_model(doc_clean,number_of_topics,words):\n",
    "\n",
    "    # Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n",
    "    dictionary = corpora.Dictionary(doc_clean)\n",
    "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "    # generate LSA model\n",
    "    ldamodel = LdaModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)\n",
    "    # train model\n",
    "    print(ldamodel.print_topics(num_topics=number_of_topics, num_words=words))\n",
    "    index = similarities.MatrixSimilarity(ldamodel[doc_term_matrix])\n",
    "    return ldamodel,dictionary,index,doc_term_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute create_gensim_lda_model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "#EXPERIMENT WITH THESE PARAMETERS\n",
    "number_of_topics=6\n",
    "words=10\n",
    "#####################################\n",
    "model2,dictionary2,index2,doctermmatrix2=create_gensim_lda_model( processed_text,number_of_topics,words)\n",
    "\n",
    "for doc in processed_text:\n",
    "    vec_bow2 = dictionary2.doc2bow(doc)\n",
    "    vec2 = model2[vec_bow2]  # convert the query to embedded space\n",
    "    sims2 = index2[vec2]  # perform a similarity query against the corpus\n",
    "    #print(list(enumerate(sims2)))  \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(30, 10))\n",
    "cax = ax.matshow(index2, interpolation='nearest')\n",
    "ax.grid(True)\n",
    "plt.xticks(range(len(processed_text)), titles, rotation=90);\n",
    "plt.yticks(range(len(processed_text)), titles);\n",
    "fig.colorbar(cax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare LSA Model Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [6, 2, 10]\n",
    "coherence_values = []\n",
    "for t in topics:\n",
    "    ldamodel,dictionary,index, matrix = create_gensim_lda_model(processed_text,t,10)\n",
    "\n",
    "\n",
    "    coherence_model_lda = CoherenceModel(model=ldamodel, dictionary=dictionary, texts=processed_text, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    coherence_values.append(coherence_lda)\n",
    "\n",
    "    \n",
    "coherence ={'6 topic 10 words':coherence_values[0],\n",
    "           '2 topic 10 words': coherence_values[1],\n",
    "           '20 topic 10 words':coherence_values[2]}   \n",
    "\n",
    "print(coherence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrate the following supervised/unsupervised learning methods:\n",
    "1. K-Means Clustering (Unsupervised)\n",
    "2. Support Vector Machines (SVMs) (Supervised)\n",
    "3. (Multinomial) Naïve Bayes (MultinomialNB) (Supervised)\n",
    "4. Logistic Regression (Supervised)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions Process Text and Create the Required TFIDF and Doc2vec Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions used to \n",
    "def clean_doc(doc): \n",
    "    #split document into individual words\n",
    "    tokens=doc.split()\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 4]\n",
    "    #lowercase all words\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]         \n",
    "    # word stemming    \n",
    "    # ps=PorterStemmer()\n",
    "    # tokens=[ps.stem(word) for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "def final_processed_text(doc):\n",
    "    #this is a function to join the processed text back\n",
    "    ' '.join(doc)\n",
    "    return doc\n",
    "\n",
    "def tfidf(corpus, titles, ngram_range = (1,1)):\n",
    "    #this is a function to created the tfidf matrix\n",
    "    Tfidf=TfidfVectorizer(ngram_range= ngram_range)\n",
    "\n",
    "    #fit the vectorizer using final processed documents.  The vectorizer requires the \n",
    "    #stiched back together document.\n",
    "\n",
    "    TFIDF_matrix=Tfidf.fit_transform(corpus)     \n",
    "\n",
    "    #creating datafram from TFIDF Matrix\n",
    "    words = Tfidf.get_feature_names()\n",
    "    matrix=pd.DataFrame(TFIDF_matrix.toarray(), columns=Tfidf.get_feature_names(), index=titles)\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def One_Hot(variable):\n",
    "    #this is a function to one hot encode the classes\n",
    "    LE=LabelEncoder()\n",
    "    LE.fit(variable)\n",
    "    Label1=LE.transform(variable)\n",
    "    OHE=OneHotEncoder()\n",
    "    labels=OHE.fit_transform(Label1.reshape(-1,1)).toarray()\n",
    "    return labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means(titles, tfidf_matrix, k=3):\n",
    "    \n",
    "    #this is a function to generate the k-means output using the tfidf matrix.  Inputs \n",
    "    #to the function include: titles of text, processed text, and desired k value. \n",
    "    #Returns dataframe indicating cluster number per document\n",
    "\n",
    "    km = KMeans(n_clusters=k, random_state =89)\n",
    "    km.fit(tfidf_matrix)\n",
    "    clusters = km.labels_.tolist()\n",
    "\n",
    "    Dictionary={'Doc Name':titles, 'Cluster':clusters,  'Text': final_processed_text}\n",
    "    frame=pd.DataFrame(Dictionary, columns=['Cluster', 'Doc Name','Text'])\n",
    "    #dictionary to store clusters and respective titles\n",
    "    cluster_title={}\n",
    "\n",
    "    #note doc2vec clusters will not have individual words due to the vector representation\n",
    "    #is based on the entire document not indvidual words. As a result, there won't be individual\n",
    "    #word outputs from each cluster.   \n",
    "    for i in range(k):\n",
    "        temp=frame[frame['Cluster']==i]\n",
    "        temp_title_list=[]\n",
    "        for title in temp['Doc Name']:\n",
    "            temp_title_list.append(title)\n",
    "        cluster_title[i]=temp_title_list\n",
    "\n",
    "    return cluster_title,clusters,frame\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot TFIDF Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tfidf_matrix(cluster_title,clusters,TFIDF_matrix):\n",
    "    # convert two components as we're plotting points in a two-dimensional plane\n",
    "    # \"precomputed\" because we provide a distance matrix\n",
    "    # we will also specify `random_state` so the plot is reproducible.\n",
    "\n",
    "\n",
    "    mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n",
    "\n",
    "    dist = 1 - cosine_similarity(TFIDF_matrix)\n",
    "\n",
    "    pos = mds.fit_transform(dist)  # shape (n_components, n_samples)\n",
    "\n",
    "    xs, ys = pos[:, 0], pos[:, 1]\n",
    "\n",
    "\n",
    "    #set up colors per clusters using a dict.  number of colors must correspond to K\n",
    "    cluster_colors = {0: 'black', 1: 'grey', 2: 'blue', 3: 'rosybrown', 4: 'firebrick', \n",
    "                      5:'red', 6:'darksalmon', 7:'sienna'}\n",
    "\n",
    "\n",
    "    #set up cluster names using a dict.  \n",
    "    cluster_dict=cluster_title\n",
    "\n",
    "    #create data frame that has the result of the MDS plus the cluster numbers and titles\n",
    "    df = pd.DataFrame(dict(x=xs, y=ys, label=clusters, title=range(0,len(clusters)))) \n",
    "\n",
    "    #group by cluster\n",
    "    groups = df.groupby('label')\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(20,20)) # set size\n",
    "    ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
    "\n",
    "    #iterate through groups to layer the plot\n",
    "    #note that I use the cluster_name and cluster_color dicts with the 'name' lookup to return the appropriate color/label\n",
    "    for name, group in groups:\n",
    "        \n",
    "        r = random.random()\n",
    "        b = random.random()\n",
    "        g = random.random()\n",
    "        color = (r, g, b)\n",
    "        \n",
    "        ax.plot(group.x, group.y, marker='o', linestyle='', ms=12,\n",
    "                label=cluster_dict[name], color=color, \n",
    "                mec='none')\n",
    "        ax.set_aspect('auto')\n",
    "        ax.tick_params(\\\n",
    "            axis= 'x',          # changes apply to the x-axis\n",
    "            which='both',      # both major and minor ticks are affected\n",
    "            bottom='off',      # ticks along the bottom edge are off\n",
    "            top='off',         # ticks along the top edge are off\n",
    "            labelbottom='on')\n",
    "        ax.tick_params(\\\n",
    "            axis= 'y',         # changes apply to the y-axis\n",
    "            which='both',      # both major and minor ticks are affected\n",
    "            left='off',      # ticks along the bottom edge are off\n",
    "            top='off',         # ticks along the top edge are off\n",
    "            labelleft='on')\n",
    "\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), prop={'size': 30})      #show legend with only 1 point\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifiers(x, y, model_type, cv = 3):\n",
    "    \n",
    "    #this function is to fit 3 different model scenarios.  Support vector machines, logistic regressions, naive bayes.\n",
    "    #svm = Support vector machin\n",
    "    #logistic = Logistic regression\n",
    "    #naive_bayes = Naive Bayes Multinomial\n",
    "    \n",
    "    #can define cv value for cross validation.\n",
    "    \n",
    "    #function returns the train test split scores of each model.\n",
    "    \n",
    "    if model_type == 'svm':\n",
    "        print(\"svm\")\n",
    "        model = SVC()\n",
    "\n",
    "    elif model_type == 'logistic':\n",
    "        print(\"logistic\")\n",
    "        model = LogisticRegression()\n",
    "\n",
    "    elif model_type == 'naive_bayes':\n",
    "        print(\"naive_bayes\")\n",
    "        model = MultinomialNB()\n",
    "    \n",
    "    elif model_type == 'randomforest':\n",
    "        print(\"randomforest\")\n",
    "        model = RandomForestClassifier()\n",
    "\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    \n",
    "    predictions = model.predict(X_test)\n",
    "    accy = accuracy_score(y_test, predictions)\n",
    "    \n",
    "    return accy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data For Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in class corpus csv into python\n",
    "data=pd.read_csv('change me /path/to/source/data/Class_Corpus_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe Store the Processed Text and Tokenized text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding two columns to the dataframe to store the processed text and tokenized text\n",
    "data['processed_text'] = data['Text'].apply(lambda x: clean_doc(x))\n",
    "\n",
    "\n",
    "\n",
    "#creating final processed text variables for matrix creation\n",
    "final_processed_text = [' '.join(x) for x in data['processed_text'].tolist()]\n",
    "titles = data['DSI_Title'].tolist()\n",
    "processed_text = data['processed_text'].tolist()\n",
    "\n",
    "\n",
    "#generate tfidf for analysis\n",
    "tfidf_matrix = tfidf(final_processed_text, titles, ngram_range = (1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute K-means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_title,clusters,k_means_df = k_means(titles, tfidf_matrix, k =20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access each cluster by changing the dictionary key value from 0 to k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_title[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot TFIDF Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_tfidf_matrix(cluster_title,clusters,tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data['Review Type (pos or neg)'].apply(lambda x: 0 if x.lower().split(' ')[0] == 'negative' else 1)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers(tfidf_matrix, labels, 'svm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers(tfidf_matrix, labels, 'logistic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers(tfidf_matrix, labels, 'naive_bayes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers(tfidf_matrix, labels, 'randomforest')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
