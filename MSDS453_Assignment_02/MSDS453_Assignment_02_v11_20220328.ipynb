{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/djp840/MSDS_453_Public/blob/main/images/NorthwesternHeader.png?raw=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSDS453 - Research Assignment 02 - Classification and Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering methods compute a similarity (or closeness) measure, such as Euclidean distance, in order to decide whether two documents are ‘similar’ or not.  \n",
    "\n",
    "Use the entire class corpus and do sentiment analysis for the positive and negative reviews.\n",
    "\n",
    "Topic modeling is another way to group ‘similar’ documents into ‘clusters’. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, similarities\n",
    "from gensim.models import Word2Vec, LdaMulticore, TfidfModel, CoherenceModel\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.models import LsiModel,LdaModel\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE, MDS\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import scipy.cluster.hierarchy\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from typing import List, Callable, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mount Google Drive to Colab Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Directories Required for Research Assignment</b>:<br> \n",
    "1. Data Directory - Source Class Corpus Data<br>\n",
    "2. Output Directory - Vocabulary<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uncomment To Map Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this once, they will be downloaded.\n",
    "nltk.download('stopwords',quiet=True)\n",
    "nltk.download('wordnet',quiet=True)\n",
    "nltk.download('punkt',quiet=True)\n",
    "nltk.download('omw-1.4',quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Gensim</b> is a Python library for topic modelling, document indexing and similarity retrieval with large corpora. Target audience is the natural language processing (NLP) and information retrieval (IR) community <br><br>\n",
    "    <b>https://pypi.org/project/gensim/ </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "pkg_resources.require(\"gensim<=3.8.3\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Genism Version: \", gensim.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Suppress warning messages</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_movie_descriptor(data: pd.DataFrame, corpus_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Adds \"Movie Description\" to the supplied dataframe, in the form {Genre}_{P|N}_{Movie Title}_{DocID}\n",
    "    \"\"\"\n",
    "    review = np.where(corpus_df['Review Type (pos or neg)'] == 'Positive', 'P', 'N')\n",
    "    data['Descriptor'] = corpus_df['Genre of Movie'] + '_' + corpus_df['Movie Title'] + '_' + review + '_' + corpus_df['Doc_ID'].astype(str)\n",
    "\n",
    "def get_corpus_df(path):\n",
    "    data = pd.read_csv(path)\n",
    "    add_movie_descriptor(data, data)\n",
    "    sorted_data = data.sort_values(['Descriptor'])\n",
    "    indexed_data = sorted_data.set_index(['Doc_ID'])\n",
    "    indexed_data['Doc_ID'] = indexed_data.index\n",
    "    return indexed_data\n",
    "\n",
    "@dataclass\n",
    "class Document:\n",
    "    doc_id: str\n",
    "    text: str\n",
    "\n",
    "def normalize_document(document: Document) -> Document:\n",
    "    text = document.text\n",
    "    text = remove_punctuation(text)\n",
    "    text = lower_case(text)\n",
    "    text = remove_tags(text)\n",
    "    text = remove_special_chars_and_digits(text)\n",
    "    \n",
    "    return Document(document.doc_id, text)\n",
    "\n",
    "def normalize_documents(documents: List[Document]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Normalizes text for all given documents.\n",
    "    Removes punctuation, converts to lower case, removes tags and special characters.\n",
    "    \"\"\"\n",
    "    return [normalize_document(x) for x in documents]\n",
    "\n",
    "@dataclass\n",
    "class TokenizedDocument:\n",
    "    doc_id: str\n",
    "    tokens: List[str]\n",
    "\n",
    "def tokenize_document(document: Document) -> TokenizedDocument:\n",
    "    tokens = nltk.word_tokenize(document.text)\n",
    "    return TokenizedDocument(document.doc_id, tokens)\n",
    "\n",
    "def tokenize_documents(documents: List[Document]) -> List[TokenizedDocument]:\n",
    "    return [tokenize_document(x) for x in documents]\n",
    "\n",
    "def lemmatize(documents: List[TokenizedDocument]) -> List[TokenizedDocument]:\n",
    "    result = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for document in documents:\n",
    "        output_tokens = [lemmatizer.lemmatize(w) for w in document.tokens]\n",
    "        result.append(TokenizedDocument(document.doc_id, output_tokens))\n",
    "        \n",
    "    return result\n",
    "\n",
    "def stem(documents: List[TokenizedDocument]) -> List[TokenizedDocument]:\n",
    "    result = []\n",
    "    stemmer = PorterStemmer()\n",
    "    for document in documents:\n",
    "        output_tokens = [stemmer.stem(w) for w in document.tokens]\n",
    "        result.append(TokenizedDocument(document.doc_id, output_tokens))\n",
    "\n",
    "    return result\n",
    "\n",
    "def remove_stop_words(documents: List[TokenizedDocument]) -> List[TokenizedDocument]:\n",
    "    result = []\n",
    "    \n",
    "    stop_words = set(nltk.corpus.stopwords.words('english')) \n",
    "    for document in documents:\n",
    "        filtered_tokens = [w for w in document.tokens if not w in stop_words]\n",
    "        result.append(TokenizedDocument(document.doc_id, filtered_tokens))\n",
    "        \n",
    "    return result\n",
    "    \n",
    "def add_flags(data: pd.DataFrame, casino_royale_doc_ids: List[int], action_doc_ids: List[int]):\n",
    "    data['is_casino_royale'] = data.index.isin(casino_royale_doc_ids)\n",
    "    data['is_action'] = data.index.isin(action_doc_ids)\n",
    "    \n",
    "def get_all_tokens(documents: List[TokenizedDocument]) -> List[str]:\n",
    "    tokens = {y for x in documents for y in x.tokens}\n",
    "    return sorted(list(tokens))\n",
    "\n",
    "def clean_doc(doc): \n",
    "    #split document into individual words\n",
    "    tokens=doc.split()\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 4]\n",
    "    #lowercase all words\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]         \n",
    "    # word stemming    \n",
    "    # ps=PorterStemmer()\n",
    "    # tokens=[ps.stem(word) for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "def final_processed_text(doc):\n",
    "    #this is a function to join the processed text back\n",
    "    ' '.join(doc)\n",
    "    return doc\n",
    "\n",
    "def tfidf(corpus, titles, ngram_range = (1,1)):\n",
    "    #this is a function to created the tfidf matrix\n",
    "    Tfidf=TfidfVectorizer(ngram_range=(1,1))\n",
    "\n",
    "    #fit the vectorizer using final processed documents.  The vectorizer requires the \n",
    "    #stiched back together document.\n",
    "\n",
    "    TFIDF_matrix=Tfidf.fit_transform(corpus)     \n",
    "\n",
    "    #creating datafram from TFIDF Matrix\n",
    "    words = Tfidf.get_feature_names()\n",
    "    matrix=pd.DataFrame(TFIDF_matrix.toarray(), columns=Tfidf.get_feature_names(), index=titles)\n",
    "    return matrix #,words\n",
    "\n",
    "def One_Hot(variable):\n",
    "    #this is a function to one hot encode the classes\n",
    "    LE=LabelEncoder()\n",
    "    LE.fit(variable)\n",
    "    Label1=LE.transform(variable)\n",
    "    OHE=OneHotEncoder()\n",
    "    labels=OHE.fit_transform(Label1.reshape(-1,1)).toarray()\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions Latent Semantic Analysis, Latent Dirichlet Allocation, Word2Vec Matrix \n",
    "1. Latent Semantic Analysis \n",
    "2. Latent Dirichlet Allocation\n",
    "3. Word2Vec Matrix creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gensim_lsa_model(doc_clean,number_of_topics,words):\n",
    "\n",
    "    # Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n",
    "    dictionary = corpora.Dictionary(doc_clean)\n",
    "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "    # generate LSA model\n",
    "    lsamodel = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)  # train model\n",
    "    print(lsamodel.print_topics(num_topics=number_of_topics, num_words=words))\n",
    "    index = similarities.MatrixSimilarity(lsamodel[doc_term_matrix])\n",
    "\n",
    "    return lsamodel,dictionary,index\n",
    "\n",
    "def lsa(tfidf_matrix, terms, n_components = 10):\n",
    "    #this is a function to execute lsa.  inputs to the function include the tfidf matrix and\n",
    "    #the desired number of components.\n",
    "    \n",
    "    LSA = TruncatedSVD(n_components=10)\n",
    "    LSA.fit(tfidf_matrix)\n",
    "\n",
    "    for i, comp in enumerate(LSA.components_):\n",
    "        terms_comp = zip(terms, comp)\n",
    "        sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:7]\n",
    "        print(\"Topic \"+str(i)+\": \")\n",
    "        for t in sorted_terms:\n",
    "            print(t[0])\n",
    "            \n",
    "def create_gensim_lda_model(doc_clean,number_of_topics,words):\n",
    "\n",
    "    # Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n",
    "    dictionary = corpora.Dictionary(doc_clean)\n",
    "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "    # generate LSA model\n",
    "    ldamodel = LdaModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)\n",
    "    # train model\n",
    "    print(ldamodel.print_topics(num_topics=number_of_topics, num_words=words))\n",
    "    index = similarities.MatrixSimilarity(ldamodel[doc_term_matrix])\n",
    "    return ldamodel,dictionary,index,doc_term_matrix\n",
    "\n",
    "def lda(tfidf_matrix, terms, topics = 3, num_words = 10):\n",
    "    #this is a function to perform lda on the tfidf matrix.  function varibales include:\n",
    "    #tfidf matrix, desired number of topic, and number of words per topic.\n",
    "\n",
    "    topics = 3\n",
    "    num_words = 10\n",
    "    lda = LatentDirichletAllocation(n_components=topics).fit(tfidf_matrix)\n",
    "\n",
    "    topic_dict = {}\n",
    "    for topic_num, topic in enumerate(lda.components_):\n",
    "        topic_dict[topic_num] = \" \".join([terms[i]for i in topic.argsort()[:-num_words - 1:-1]])\n",
    "\n",
    "    print(topic_dict)\n",
    "    \n",
    "def word2vec(processed_text, size = 100):\n",
    "    #This is a function to generate the word2vec matrix. Input parameters include the \n",
    "    #tokenized text and matrix size\n",
    "    \n",
    "    #word to vec\n",
    "    model_w2v = Word2Vec(processed_text, size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "    #join all processed DSI words into single list\n",
    "    processed_text_w2v=[]\n",
    "    for i in processed_text:\n",
    "        for k in i:\n",
    "            processed_text_w2v.append(k)\n",
    "\n",
    "    #obtian all the unique words from DSI\n",
    "    w2v_words=list(set(processed_text_w2v))\n",
    "\n",
    "    #can also use the get_feature_names() from TFIDF to get the list of words\n",
    "    #w2v_words=Tfidf.get_feature_names()\n",
    "\n",
    "    #empty dictionary to store words with vectors\n",
    "    w2v_vectors={}\n",
    "\n",
    "    #for loop to obtain weights for each word\n",
    "    for i in w2v_words:\n",
    "        temp_vec=model_w2v.wv[i]\n",
    "        w2v_vectors[i]=temp_vec\n",
    "\n",
    "    #create a final dataframe to view word vectors\n",
    "    w2v_df=pd.DataFrame(w2v_vectors).transpose()\n",
    "    print(w2v_df)\n",
    "    return w2v_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Functions: KMeans, SVM, Logistic, Naive Bayes, Randomforest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means(titles, tfidf_matrix, k=3):\n",
    "    \n",
    "    #this is a function to generate the k-means output using the tfidf matrix.  Inputs \n",
    "    #to the function include: titles of text, processed text, and desired k value. \n",
    "    #Returns dataframe indicating cluster number per document\n",
    "\n",
    "    km = KMeans(n_clusters=k, random_state =89)\n",
    "    km.fit(tfidf_matrix)\n",
    "    clusters = km.labels_.tolist()\n",
    "\n",
    "    Dictionary={'Doc Name':titles, 'Cluster':clusters,  'Text': final_processed_text}\n",
    "    frame=pd.DataFrame(Dictionary, columns=['Cluster', 'Doc Name','Text'])\n",
    "    #dictionary to store clusters and respective titles\n",
    "    cluster_title={}\n",
    "\n",
    "    #note doc2vec clusters will not have individual words due to the vector representation\n",
    "    #is based on the entire document not indvidual words. As a result, there won't be individual\n",
    "    #word outputs from each cluster.   \n",
    "    for i in range(k):\n",
    "        temp=frame[frame['Cluster']==i]\n",
    "        temp_title_list=[]\n",
    "        for title in temp['Doc Name']:\n",
    "            temp_title_list.append(title)\n",
    "        cluster_title[i]=temp_title_list\n",
    "\n",
    "    return cluster_title,clusters,frame\n",
    "\n",
    "def classifiers(x, y, model_type, cv = 3):\n",
    "    \n",
    "    #this function is to fit 3 different model scenarios.  Support vector machines, logistic regressions, naive bayes.\n",
    "    #svm = Support vector machin\n",
    "    #logistic = Logistic regression\n",
    "    #naive_bayes = Naive Bayes Multinomial\n",
    "    \n",
    "    #can define cv value for cross validation.\n",
    "    \n",
    "    #function returns the train test split scores of each model.\n",
    "    \n",
    "    if model_type == 'svm':\n",
    "        print(\"svm\")\n",
    "        model = SVC()\n",
    "\n",
    "    elif model_type == 'logistic':\n",
    "        print(\"logistic\")\n",
    "        model = LogisticRegression()\n",
    "\n",
    "    elif model_type == 'naive_bayes':\n",
    "        print(\"naive_bayes\")\n",
    "        model = MultinomialNB()\n",
    "    \n",
    "    elif model_type == 'randomforest':\n",
    "        print(\"randomforest\")\n",
    "        model = RandomForestClassifier()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    predictions = model.predict(X_test)\n",
    "    accy = accuracy_score(y_test, predictions) \n",
    "    return accy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lsa(number_of_topics, words):\n",
    "\n",
    "    model,dictionary,index=create_gensim_lsa_model(processed_text,number_of_topics,words)\n",
    "\n",
    "    for doc in processed_text:\n",
    "        vec_bow = dictionary.doc2bow(doc)\n",
    "        vec_lsi = model[vec_bow]  # convert the query to LSI space\n",
    "        sims = index[vec_lsi] # perform a similarity query against the corpus\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(30, 10))\n",
    "    cax = ax.matshow(index, interpolation='nearest')\n",
    "    ax.grid(True)\n",
    "    plt.xticks(range(len(processed_text)), titles, rotation=90);\n",
    "    plt.yticks(range(len(processed_text)), titles);\n",
    "    fig.colorbar(cax)\n",
    "    plt.show()\n",
    "    return model\n",
    "\n",
    "def plot_tfidf_matrix(cluster_title,clusters,TFIDF_matrix):\n",
    "    # convert two components as we're plotting points in a two-dimensional plane\n",
    "    # \"precomputed\" because we provide a distance matrix\n",
    "    # we will also specify `random_state` so the plot is reproducible.\n",
    "\n",
    "    mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n",
    "    dist = 1 - cosine_similarity(TFIDF_matrix)\n",
    "    pos = mds.fit_transform(dist)  # shape (n_components, n_samples)\n",
    "    xs, ys = pos[:, 0], pos[:, 1]\n",
    "\n",
    "\n",
    "    #set up colors per clusters using a dict.  number of colors must correspond to K\n",
    "    cluster_colors = {0: 'black', 1: 'grey', 2: 'blue', 3: 'rosybrown', 4: 'firebrick', \n",
    "                      5:'red', 6:'darksalmon', 7:'sienna'}\n",
    "\n",
    "    #set up cluster names using a dict.  \n",
    "    cluster_dict=cluster_title\n",
    "\n",
    "    #create data frame that has the result of the MDS plus the cluster numbers and titles\n",
    "    df = pd.DataFrame(dict(x=xs, y=ys, label=clusters, title=range(0,len(clusters)))) \n",
    "\n",
    "    #group by cluster\n",
    "    groups = df.groupby('label')\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(20,20)) # set size\n",
    "    ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
    "\n",
    "    #iterate through groups to layer the plot\n",
    "    #note that I use the cluster_name and cluster_color dicts with the 'name' lookup to return the appropriate color/label\n",
    "    for name, group in groups:\n",
    "        \n",
    "        r = random.random()\n",
    "        b = random.random()\n",
    "        g = random.random()\n",
    "        color = (r, g, b)\n",
    "        \n",
    "        ax.plot(group.x, group.y, marker='o', linestyle='', ms=12,\n",
    "                label=cluster_dict[name], color=color, \n",
    "                mec='none')\n",
    "        ax.set_aspect('auto')\n",
    "        ax.tick_params(\\\n",
    "            axis= 'x',          # changes apply to the x-axis\n",
    "            which='both',      # both major and minor ticks are affected\n",
    "            bottom='off',      # ticks along the bottom edge are off\n",
    "            top='off',         # ticks along the top edge are off\n",
    "            labelbottom='on')\n",
    "        ax.tick_params(\\\n",
    "            axis= 'y',         # changes apply to the y-axis\n",
    "            which='both',      # both major and minor ticks are affected\n",
    "            left='off',      # ticks along the bottom edge are off\n",
    "            top='off',         # ticks along the top edge are off\n",
    "            labelleft='on')\n",
    "\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), prop={'size': 30})      #show legend with only 1 point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Class Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in class corpus csv into python\n",
    "data=pd.read_csv(r'./data/MSDS453_Sec57_ClassCorpus_v4.csv')\n",
    "\n",
    "#adding two columns to the dataframe to store the processed text and tokenized text\n",
    "data['processed_text'] = data['Text'].apply(lambda x: clean_doc(x))\n",
    "\n",
    "#creating final processed text variables for matrix creation\n",
    "final_processed_text = [' '.join(x) for x in data['processed_text'].tolist()]\n",
    "titles = data['DSI_Title'].tolist()\n",
    "processed_text = data['processed_text'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Dataframe = corpus_df\n",
    "2. List = documents (Document ID, Text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_PATH = './data/MSDS453_QA_TEST.csv'\n",
    "corpus_df = get_corpus_df(CORPUS_PATH)\n",
    "documents = [Document(x, y) for x, y in zip(corpus_df.Doc_ID, corpus_df.Text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus_df.info());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Semantic Analysis - Research Number of Topics and Number of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting 2 topics and 10 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2topic_10words = plot_lsa(2, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting 6 topics and 10 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_6topic_10words = plot_lsa(6, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting 20 topics and 10 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_20topic_10words =plot_lsa(20, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare LSA Model Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [6, 2, 10]\n",
    "coherence_values = []\n",
    "for t in topics:\n",
    "    lsamodel,dictionary,index = create_gensim_lsa_model(processed_text,t,10)\n",
    "\n",
    "\n",
    "    coherence_model_lsa = CoherenceModel(model=lsamodel, dictionary=dictionary, texts=processed_text, coherence='c_v')\n",
    "    coherence_lsa = coherence_model_lsa.get_coherence()\n",
    "    coherence_values.append(coherence_lsa)\n",
    "\n",
    "    \n",
    "coherence ={'6 topic 10 words':coherence_values[0],\n",
    "           '2 topic 10 words': coherence_values[1],\n",
    "           '20 topic 10 words':coherence_values[2]}   \n",
    "\n",
    "print(coherence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation - Research Number of Topics and Number of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "#EXPERIMENT WITH THESE PARAMETERS\n",
    "number_of_topics=6\n",
    "words=10\n",
    "#####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2,dictionary2,index2,doctermmatrix2=create_gensim_lda_model( processed_text,number_of_topics,words)\n",
    "\n",
    "for doc in processed_text:\n",
    "    vec_bow2 = dictionary2.doc2bow(doc)\n",
    "    vec2 = model2[vec_bow2]  # convert the query to embedded space\n",
    "    sims2 = index2[vec2]  # perform a similarity query against the corpus\n",
    "    #print(list(enumerate(sims2)))  \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(30, 10))\n",
    "cax = ax.matshow(index2, interpolation='nearest')\n",
    "ax.grid(True)\n",
    "plt.xticks(range(len(processed_text)), titles, rotation=90);\n",
    "plt.yticks(range(len(processed_text)), titles);\n",
    "fig.colorbar(cax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare LDA Model Coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [6, 2, 10]\n",
    "coherence_values = []\n",
    "for t in topics:\n",
    "    ldamodel,dictionary,index, matrix = create_gensim_lda_model(processed_text,t,10)\n",
    "\n",
    "\n",
    "    coherence_model_lda = CoherenceModel(model=ldamodel, dictionary=dictionary, texts=processed_text, coherence='c_v')\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\n",
    "    coherence_values.append(coherence_lda)\n",
    "\n",
    "    \n",
    "coherence ={'6 topic 10 words':coherence_values[0],\n",
    "           '2 topic 10 words': coherence_values[1],\n",
    "           '20 topic 10 words':coherence_values[2]}   \n",
    "\n",
    "print(coherence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Supervised and Unsupervised Learning Methods:\n",
    "1. K-Means Clustering (Unsupervised)\n",
    "2. Support Vector Machines (SVMs) (Supervised)\n",
    "3. (Multinomial) Naïve Bayes (MultinomialNB) (Supervised)\n",
    "4. Logistic Regression (Supervised)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Model Research: KMeans, SVM, Logistic, Naive Bayes, Randomforest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate TF-IDF For Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix = tfidf(final_processed_text, titles, ngram_range = (1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>sklearn.cluster.KMeans</b>:<br> \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html<br><br>\n",
    "<b>KMeans Default Parameters</b>:<br>\n",
    "    class sklearn.cluster.KMeans(n_clusters=8, *, init='k-means++', n_init=10, max_iter=300, tol=0.0001, verbose=0, random_state=None, copy_x=True, algorithm='auto')\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_title,clusters,k_means_df = k_means(titles, tfidf_matrix, k =20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access Each Cluster - Changing Dictionary KeyValue from 0 to k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_title[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tfidf_matrix(cluster_title,clusters,tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Labels for Supervised Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data['Review Type (pos or neg)'].apply(lambda x: 0 if x.lower().split(' ')[0] == 'negative' else 1)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Support vector machines (SVMs)</b>:<br> \n",
    "https://scikit-learn.org/stable/modules/svm.html<br><br>\n",
    "    Set of supervised learning methods used for classification, regression and outliers detection\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers(tfidf_matrix, labels, 'svm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>sklearn.linear_model.LogisticRegression</b>:<br> \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html<br><br>\n",
    "<b>Logistic Regression Default Parameters</b>:<br>\n",
    "    class sklearn.linear_model.LogisticRegression(penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers(tfidf_matrix, labels, 'logistic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Naive Bayes</b>:<br> \n",
    "https://scikit-learn.org/stable/modules/naive_bayes.html<br><br>\n",
    "    Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes’ theorem with the “naive” assumption of conditional independence between every pair of features given the value of the class variable.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers(tfidf_matrix, labels, 'naive_bayes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>sklearn.ensemble.RandomForestClassifier</b>:<br> \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html<br><br>\n",
    "<b>RandomForest Classifier Default Parameters</b>:<br>\n",
    "    class sklearn.ensemble.RandomForestClassifier(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers(tfidf_matrix, labels, 'randomforest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
