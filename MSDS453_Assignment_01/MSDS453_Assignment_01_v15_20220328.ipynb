{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/djp840/MSDS_453_Public/blob/main/images/NorthwesternHeader.png?raw=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSDS453 - Research Assignment 01 - First Vectorized Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Our goal in this exercise is to BEGIN coming to a common agreement, among this class, as to what terms we will use as we selectively refine our corpus-wide vocabulary. This corpus vocabulary is what would represent the content of each different document for clustering and classification purposes, which will be our next step. This means that we need to make decisions - what is in, what is out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import scipy.cluster.hierarchy\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from typing import List, Callable, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mount Google Drive to Colab Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Directories Required for Research Assignment</b>:<br> \n",
    "1. Data Directory - Source Class Corpus Data<br>\n",
    "2. Output Directory - Vocabulary<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uncomment To Map Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this once, they will be downloaded.\n",
    "nltk.download('stopwords',quiet=True)\n",
    "nltk.download('wordnet',quiet=True)\n",
    "nltk.download('punkt',quiet=True)\n",
    "nltk.download('omw-1.4',quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Gensim</b> is a Python library for topic modelling, document indexing and similarity retrieval with large corpora. Target audience is the natural language processing (NLP) and information retrieval (IR) community <br><br>\n",
    "    <b>https://pypi.org/project/gensim/ </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "pkg_resources.require(\"gensim<=3.8.3\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Genism Version: \", gensim.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Suppress warning messages</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_movie_descriptor(data: pd.DataFrame, corpus_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Adds \"Movie Description\" to the supplied dataframe, in the form {Genre}_{P|N}_{Movie Title}_{DocID}\n",
    "    \"\"\"\n",
    "    review = np.where(corpus_df['Review Type (pos or neg)'] == 'Positive', 'P', 'N')\n",
    "    data['Descriptor'] = corpus_df['Genre of Movie'] + '_' + corpus_df['Movie Title'] + '_' + review + '_' + corpus_df['Doc_ID'].astype(str)\n",
    "\n",
    "def get_corpus_df(path):\n",
    "    data = pd.read_csv(path)\n",
    "    add_movie_descriptor(data, data)\n",
    "    sorted_data = data.sort_values(['Descriptor'])\n",
    "    indexed_data = sorted_data.set_index(['Doc_ID'])\n",
    "    indexed_data['Doc_ID'] = indexed_data.index\n",
    "    return indexed_data\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return re.sub('[^a-zA-Z]', ' ', str(text))\n",
    "\n",
    "def lower_case(text):\n",
    "    return text.lower()    \n",
    "\n",
    "def remove_tags(text):    \n",
    "    return re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \", text)\n",
    "\n",
    "def remove_special_chars_and_digits(text):\n",
    "    return re.sub(\"(\\\\d|\\\\W)+\",\" \", text)\n",
    "\n",
    "@dataclass\n",
    "class Document:\n",
    "    doc_id: str\n",
    "    text: str\n",
    "\n",
    "def normalize_document(document: Document) -> Document:\n",
    "    text = document.text\n",
    "    text = remove_punctuation(text)\n",
    "    text = lower_case(text)\n",
    "    text = remove_tags(text)\n",
    "    text = remove_special_chars_and_digits(text)\n",
    "    \n",
    "    return Document(document.doc_id, text)\n",
    "\n",
    "def normalize_documents(documents: List[Document]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Normalizes text for all given documents.\n",
    "    Removes punctuation, converts to lower case, removes tags and special characters.\n",
    "    \"\"\"\n",
    "    return [normalize_document(x) for x in documents]\n",
    "\n",
    "@dataclass\n",
    "class TokenizedDocument:\n",
    "    doc_id: str\n",
    "    tokens: List[str]\n",
    "\n",
    "def tokenize_document(document: Document) -> TokenizedDocument:\n",
    "    tokens = nltk.word_tokenize(document.text)\n",
    "    return TokenizedDocument(document.doc_id, tokens)\n",
    "\n",
    "def tokenize_documents(documents: List[Document]) -> List[TokenizedDocument]:\n",
    "    return [tokenize_document(x) for x in documents]\n",
    "\n",
    "def lemmatize(documents: List[TokenizedDocument]) -> List[TokenizedDocument]:\n",
    "    result = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for document in documents:\n",
    "        output_tokens = [lemmatizer.lemmatize(w) for w in document.tokens]\n",
    "        result.append(TokenizedDocument(document.doc_id, output_tokens))\n",
    "        \n",
    "    return result\n",
    "\n",
    "def stem(documents: List[TokenizedDocument]) -> List[TokenizedDocument]:\n",
    "    result = []\n",
    "    stemmer = PorterStemmer()\n",
    "    for document in documents:\n",
    "        output_tokens = [stemmer.stem(w) for w in document.tokens]\n",
    "        result.append(TokenizedDocument(document.doc_id, output_tokens))\n",
    "\n",
    "    return result\n",
    "\n",
    "def remove_stop_words(documents: List[TokenizedDocument]) -> List[TokenizedDocument]:\n",
    "    result = []\n",
    "    \n",
    "    stop_words = set(nltk.corpus.stopwords.words('english')) \n",
    "    for document in documents:\n",
    "        filtered_tokens = [w for w in document.tokens if not w in stop_words]\n",
    "        result.append(TokenizedDocument(document.doc_id, filtered_tokens))\n",
    "        \n",
    "    return result\n",
    "    \n",
    "def add_flags(data: pd.DataFrame, casino_royale_doc_ids: List[int], action_doc_ids: List[int]):\n",
    "    data['is_casino_royale'] = data.index.isin(casino_royale_doc_ids)\n",
    "    data['is_action'] = data.index.isin(action_doc_ids)\n",
    "    \n",
    "def get_all_tokens(documents: List[TokenizedDocument]) -> List[str]:\n",
    "    tokens = {y for x in documents for y in x.tokens}\n",
    "    return sorted(list(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_PATH = './data/MSDS453_QA_TEST.csv'\n",
    "corpus_df = get_corpus_df(CORPUS_PATH)\n",
    "documents = [Document(x, y) for x, y in zip(corpus_df.Doc_ID, corpus_df.Text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Dataframe = corpus_df\n",
    "2. List = documents (Document ID, Text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus_df.info());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Reviews By Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_df = corpus_df[['Genre of Movie']].copy()\n",
    "counts_df['Count'] = 1\n",
    "counts_df.groupby(['Genre of Movie']).count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalized Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. remove_punctuation(text)\n",
    "2. lower_case(text)\n",
    "3. remove_tags(text)\n",
    "4. remove_special_chars_and_digits(text)\n",
    "5. return Document(document.doc_id, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_documents = normalize_documents(documents)\n",
    "normalized_documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenized Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK Tokenizer Package\n",
    "\n",
    "https://www.nltk.org/api/nltk.tokenize.html\n",
    "\n",
    "Tokenizers divide strings into lists of substrings. For example, tokenizers can be used to find the words and punctuation in a string:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Tokenization Process\n",
    "1. tokenize_document\n",
    "2. tokenize_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_documents = tokenize_documents(normalized_documents)\n",
    "tokenized_documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Lookups (Titles by DocID, Genres by DocID, Description by DocID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_by_doc_ids = {x: y for x, y in zip(corpus_df['Doc_ID'], corpus_df['Movie Title'])}\n",
    "genres_by_doc_ids = {x: y for x, y in zip(corpus_df['Doc_ID'], corpus_df['Genre of Movie'])}\n",
    "descriptors_by_doc_ids = {x: y for x, y in zip(corpus_df['Doc_ID'], corpus_df['Descriptor'])}\n",
    "\n",
    "action_doc_ids = [int(x) for x in corpus_df['Doc_ID'] if genres_by_doc_ids[x] == 'Action']\n",
    "action_documents = [x for x in documents if x.doc_id in action_doc_ids]\n",
    "\n",
    "non_action_doc_ids = {int(x) for x in corpus_df['Doc_ID'] if genres_by_doc_ids[x] != 'Action'}\n",
    "non_action_documents = [x for x in documents if x.doc_id in non_action_doc_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lookup for Specific Movie Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus_df['Movie Title'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "casino_royale_doc_ids = [int(x) for x in corpus_df['Doc_ID'] if titles_by_doc_ids[x] == 'Casino_Royale']\n",
    "casino_royale_documents = [x for x in documents if x.doc_id in casino_royale_doc_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative Term Determinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Terms Determined by Document of Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_terms = [\n",
    "    'action',\n",
    "    'sequence',\n",
    "    'grit',\n",
    "    'plot',\n",
    "    'chase',\n",
    "    'airline',\n",
    "    'stock',\n",
    "    'series',\n",
    "    'effect',\n",
    "    'strong',\n",
    "    'theme',\n",
    "    'character',\n",
    "    'money',\n",
    "    'spy'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>sklearn.feature_extraction.text.CountVectorizer</b>:<br> \n",
    "Convert a collection of text documents to a matrix of token counts.<br>\n",
    "This implementation produces a sparse representation of the counts using scipy.sparse.csr_matrix.<br>\n",
    "If you do not provide an a-priori dictionary and you do not use an analyzer that does some kind of feature selection then the number of features will be equal to the vocabulary size found by analyzing the data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1, 1))\n",
    "text_for_counts = [x.text for x in normalized_documents]\n",
    "matrix = vectorizer.fit_transform(text_for_counts)\n",
    "words = vectorizer.get_feature_names_out()\n",
    "word_counts = pd.DataFrame(matrix.toarray(), columns=words, index=corpus_df.Doc_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_flags(word_counts, casino_royale_doc_ids, action_doc_ids)\n",
    "word_counts['Doc_ID'] = word_counts.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect result into a dataframe\n",
    "mean_frequencies = pd.DataFrame(index=candidate_terms)\n",
    "\n",
    "casino_royale_mean_frequencies = word_counts[word_counts.is_casino_royale][[x for x in candidate_terms if x in word_counts.columns]].mean()\n",
    "mean_frequencies['Casino Royale'] = casino_royale_mean_frequencies\n",
    "\n",
    "action_mean_frequencies = word_counts[word_counts.is_action][[x for x in candidate_terms if x in word_counts.columns]].mean()\n",
    "mean_frequencies['All Action'] = action_mean_frequencies\n",
    "\n",
    "non_action_mean_frequencies = word_counts[~word_counts.is_action][[x for x in candidate_terms if x in word_counts.columns]].mean()\n",
    "mean_frequencies['All Non-Action'] = non_action_mean_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_frequencies.fillna(0.0).sort_values(['Casino Royale'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are identified\n",
    "important_prevalent_terms = [\n",
    "    'action',\n",
    "    'series',\n",
    "    'money',\n",
    "    'chase'\n",
    "]\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_important_prevalent_terms = [stemmer.stem(x) for x in important_prevalent_terms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "mean_frequencies.fillna(0.0).loc[important_prevalent_terms].round(2).sort_values(['Casino Royale'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Experiment Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tfidf(documents: List[Document],\n",
    "              clean_func: Callable[[List[Document]], List[TokenizedDocument]],\n",
    "              important_prevalent_terms: List[str],\n",
    "              experiment_name: str,\n",
    "              output_tfidf_vectors: bool=False,\n",
    "              output_vocabulary: bool=True):\n",
    "    cleaned_documents = clean_func(documents)\n",
    "    cleaned_document_text = [' '.join(x.tokens) for x in cleaned_documents]\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(use_idf=True,\n",
    "                                 ngram_range=(1, 1),\n",
    "                                 norm=None)\n",
    "\n",
    "    transformed_documents = vectorizer.fit_transform(cleaned_document_text)\n",
    "    transformed_documents_as_array = transformed_documents.toarray()\n",
    "\n",
    "    output_dir = f'output/{experiment_name}_Results'\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    if output_tfidf_vectors:\n",
    "        for counter, doc in enumerate(transformed_documents_as_array):\n",
    "            tf_idf_tuples = list(zip(vectorizer.get_feature_names_out(), doc))\n",
    "            one_doc_as_df = pd.DataFrame.from_records(tf_idf_tuples, columns=['term', 'score'])\\\n",
    "                                        .sort_values(by='score', ascending=False)\\\n",
    "                                        .reset_index(drop=True)\n",
    "\n",
    "            one_doc_as_df.to_csv(f'{output_dir}/{corpus_df[\"Submission File Name\"][counter]}')\n",
    "    \n",
    "    if output_vocabulary:\n",
    "        with open(f'{output_dir}/vocabulary.txt', 'w') as vocab:\n",
    "            words = sorted(vectorizer.get_feature_names_out())\n",
    "            print('\\n'.join(words), file=vocab)\n",
    "\n",
    "    # Create document-term dataframe\n",
    "    doc_term_matrix = transformed_documents.todense()\n",
    "    doc_term_df = pd.DataFrame(doc_term_matrix, \n",
    "                               columns=vectorizer.get_feature_names_out(), \n",
    "                               index=corpus_df.Doc_ID)\n",
    "    add_flags(doc_term_df, casino_royale_doc_ids, action_doc_ids)\n",
    "    \n",
    "    # Print the top 10 mean TF-IDF values\n",
    "    top10_tfidf = pd.DataFrame(doc_term_df.mean().sort_values(ascending=False).head(10))\n",
    "    top10_tfidf.rename(columns={0: 'Mean TF-IDF'}, inplace=True)\n",
    "    display(top10_tfidf)\n",
    "    \n",
    "    # Collect result into a dataframe\n",
    "    tfidf_results = pd.DataFrame(index=important_prevalent_terms)\n",
    " \n",
    "    all_tfidf_results = doc_term_df[[x for x in important_prevalent_terms if x in doc_term_df.columns]].mean().round(2)\n",
    "    tfidf_results['All Movies'] = all_tfidf_results\n",
    "    \n",
    "    plt.hist(doc_term_df.mean(), 100, range=(0, 8))\n",
    "    \n",
    "    print(f'Vocabulary size: {doc_term_df.shape[1]}')\n",
    "    \n",
    "    descriptors = corpus_df['Descriptor']\n",
    "       \n",
    "    similarities = cosine_similarity(doc_term_df.loc[action_doc_ids], doc_term_df.loc[action_doc_ids])\n",
    "    fig, ax = plt.subplots(figsize=(30, 30))\n",
    "    labels = [descriptors_by_doc_ids[x.doc_id] for x in action_documents]\n",
    "    sns.heatmap(ax=ax, data=similarities, xticklabels=labels, yticklabels=labels)\n",
    "    #plt.savefig(f'figures/{experiment_name}_heatmap_documents.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF  (Term Frequency-Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments, Normalize, Tokenize, Lemmatization and Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_method(documents: List[Document]) -> List[TokenizedDocument]:\n",
    "    \"\"\"\n",
    "    Normalizes text, tokenizes, lemmatizes, and removes stop words.\n",
    "    \"\"\"\n",
    "    documents = normalize_documents(documents)\n",
    "    documents = tokenize_documents(documents)\n",
    "    documents = lemmatize(documents)\n",
    "    documents = remove_stop_words(documents)\n",
    "    documents = stem(documents)\n",
    "        \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run_tfidf(documents, clean_method, important_prevalent_terms, 'TFIDF_exp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Word2vec embeddings</b>: https://radimrehurek.com/gensim/models/word2vec.html <br>\n",
    "    This module implements the word2vec family of algorithms, using highly optimized C routines, data streaming and Pythonic interfaces.\n",
    "            </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions For Word2Vec Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2vec_vectors(documents: List[TokenizedDocument], embedding_size: int) -> pd.DataFrame:\n",
    "    tokens = [x.tokens for x in documents]\n",
    "    \n",
    "    word2vec_model = Word2Vec(tokens, size=embedding_size, window=3, min_count=1, workers=12)\n",
    "    \n",
    "    vectors = {}\n",
    "    for i in word2vec_model.wv.vocab:\n",
    "        temp_vec = word2vec_model.wv[i]\n",
    "        vectors[i] = temp_vec\n",
    "\n",
    "    result = pd.DataFrame(vectors).transpose()\n",
    "    result = result.sort_index()\n",
    "    return result\n",
    "\n",
    "def plot_dendrogram(data: pd.DataFrame, experiment_name: str, figsize=(30, 60)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.title(\"Dendrograms\")  \n",
    "\n",
    "    linkage = scipy.cluster.hierarchy.linkage(data, method='ward')\n",
    "    scipy.cluster.hierarchy.dendrogram(linkage, labels=data.index, leaf_font_size=15, orientation='right')\n",
    "    #plt.savefig(f'figures/{experiment_name}_dendrogram.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "def plot_similarity_matrix(data: pd.DataFrame, experiment_name: str, figsize=(25, 25)):\n",
    "    similarities = cosine_similarity(data, data)\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    sns.heatmap(ax=ax, data=similarities, xticklabels=data.index, yticklabels=data.index);\n",
    "    #plt.savefig(f'figures/{experiment_name}_heatmap.png')\n",
    "    plt.close()\n",
    "    \n",
    "def plot_similarity_clustermap(data: pd.DataFrame, experiment_name: str, figsize=(25, 25)):\n",
    "    similarities = cosine_similarity(data, data)\n",
    "    cm = sns.clustermap(similarities, metric='cosine', xticklabels=data.index, yticklabels=data.index, method='complete', cmap='RdBu', figsize=figsize)\n",
    "    cm.ax_row_dendrogram.set_visible(False)\n",
    "    cm.ax_col_dendrogram.set_visible(False)\n",
    "    plt.legend(loc='upper left')\n",
    "    #plt.savefig(f'figures/{experiment_name}_clustermap.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def plot_tsne(data: pd.DataFrame, perplexity: int, experiment_name: str, figsize=(40, 40)):\n",
    "    \"\"\"\n",
    "    Creates a TSNE plot of the supplied dataframe\n",
    "    \"\"\"\n",
    "    tsne_model = TSNE(perplexity=perplexity, n_components=2, learning_rate='auto', init='pca', n_iter=1000, random_state=32)\n",
    "    new_values = tsne_model.fit_transform(data)\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=figsize)\n",
    "    labels = list(data.index)\n",
    "    for i in range(len(x)):\n",
    "        new_value = new_values[i]\n",
    "        x = new_value[0]\n",
    "        y = new_value[1]\n",
    "        \n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x, y),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    #plt.savefig(f'figures/{experiment_name}_tsne.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "def run_word2vec_experiment(documents: List[Document],\n",
    "                            clean_func: Callable[[List[Document]], List[TokenizedDocument]],\n",
    "                            embedding_size: int,\n",
    "                            chosen_tokens: List[str],\n",
    "                            experiment_name: str):\n",
    "    cleaned_documents = clean_func(documents)\n",
    "\n",
    "    word2vec_df = get_word2vec_vectors(cleaned_documents, embedding_size)\n",
    "    filtered_word2vec_df = word2vec_df.loc[chosen_tokens].copy()\n",
    "\n",
    "    plot_tsne(filtered_word2vec_df, 30, experiment_name)\n",
    "    plot_dendrogram(filtered_word2vec_df, experiment_name)\n",
    "    plot_similarity_matrix(filtered_word2vec_df, experiment_name)\n",
    "    plot_similarity_clustermap(filtered_word2vec_df, experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_terms = [\n",
    "    'bond',\n",
    "    'craig',\n",
    "    'star',\n",
    "    'casino',\n",
    "    'action'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our terms to examine in experiements 4-12\n",
    "all_tokens = get_all_tokens(remove_stop_words(clean_method(documents)))\n",
    "chosen_tokens = random.choices(all_tokens, k=100 - len(extra_terms)) + extra_terms\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_chosen_tokens = [lemmatizer.lemmatize(x) for x in chosen_tokens]\n",
    "stemmed_chosen_tokens = [stemmer.stem(x) for x in lemmatized_chosen_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Experiments: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Word2vec embeddings</b>: https://radimrehurek.com/gensim/models/word2vec.html <br>\n",
    "    This module implements the word2vec family of algorithms, using highly optimized C routines, data streaming and Pythonic interfaces.\n",
    "            </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run_word2vec_experiment(documents, clean_method, 100, chosen_tokens, 'Word2Vec_exp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions for Doc2Vec experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Doc2vec Paragraph Embeddings</b>: https://radimrehurek.com/gensim/models/doc2vec.html <br>\n",
    "    Paragraph and document embeddings via the distributed memory and distributed bag of words models from Quoc Le and Tomas Mikolov: “Distributed Representations of Sentences and Documents”. <br>\n",
    "\n",
    "The algorithms use either hierarchical softmax or negative sampling;\n",
    "            </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_doc2vec(documents: List[TokenizedDocument], embedding_size: int, descriptors_by_doc_ids: Dict[int, str]):\n",
    "    tagged_documents = [TaggedDocument(document.tokens, [i]) for i, document in enumerate(documents)]\n",
    "    doc2vec_model = Doc2Vec(tagged_documents, vector_size=embedding_size, window=3, min_count=2, workers=12)    \n",
    "    \n",
    "    doc2vec_df = pd.DataFrame()\n",
    "    for document in documents:\n",
    "        vector = pd.DataFrame(doc2vec_model.infer_vector(document.tokens)).transpose()\n",
    "        doc2vec_df = pd.concat([doc2vec_df, vector], axis=0)\n",
    "    \n",
    "    doc2vec_df['Descriptor'] = [descriptors_by_doc_ids[x.doc_id] for x in documents]\n",
    "    doc2vec_df.set_index(['Descriptor'], inplace=True)\n",
    "    return doc2vec_df\n",
    "\n",
    "def run_doc2vec_experiment(documents: List[Document],\n",
    "                           clean_func: Callable[[List[Document]], List[TokenizedDocument]],\n",
    "                           embedding_size: int,\n",
    "                           experiment_name: str):\n",
    "    cleaned_documents = clean_func(documents)\n",
    "    doc2vec_df = run_doc2vec(cleaned_documents, embedding_size, descriptors_by_doc_ids)\n",
    "    \n",
    "    plot_similarity_matrix(doc2vec_df, experiment_name)\n",
    "    plot_similarity_clustermap(doc2vec_df, experiment_name, figsize=(50, 50))\n",
    "    plot_dendrogram(doc2vec_df, experiment_name)\n",
    "    plot_tsne(doc2vec_df, 30, experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec Experiments:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Doc2vec Paragraph Embeddings</b>: https://radimrehurek.com/gensim/models/doc2vec.html <br>\n",
    "    Paragraph and document embeddings via the distributed memory and distributed bag of words models from Quoc Le and Tomas Mikolov: “Distributed Representations of Sentences and Documents”. <br>\n",
    "\n",
    "The algorithms use either hierarchical softmax or negative sampling;\n",
    "            </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run_doc2vec_experiment(documents, clean_method, 100, 'Doc2Vec_exp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
